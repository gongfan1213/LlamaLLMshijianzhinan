### 第1章 大模型的基础理论

自从OpenAI推出GPT-3以来，AI领域的发展进入一个崭新的阶段，即大模型时代。大规模语言模型又称大语言模型（Large Language Model，LLM），简称大模型，具备强大的理解和推理能力，使其能够出色地完成各种不同的任务。通过对话这种经典的人机交互方式为用户提供服务，就是大模型的一项重要应用。大模型已经成为AI领域最热门的话题之一，并被视为实现通用人工智能（Artificial General Intelligence，AGI）的可能途径。

本章主要介绍大模型的发展历史、核心框架、数据收集和数据处理、预训练及微调、评测。
#### 1.1 大模型的发展历史

大模型是自然语言处理（Natural Language Processing，NLP）领域的一个重要概念，代表了一种基于大规模语料库训练的语言模型。虽然大模型可以完成多种多样的任务，但其本质上仍然是一种语言模型，即对一个句子（由多个单词组成的序列）出现概率的建模。

##### 1.1.1 从语言模型到预训练大模型

1. **语言模型**

![image](https://github.com/user-attachments/assets/e5b80e90-336a-4659-94d4-39bcdd055d58)


语言模型是NLP领域的一个核心概念，是对语言的规律和分布的统计建模，用于推断给定序列中一个词的分布概率。语言模型的形式化定义如下，对于由$l$个词（词元，即语言的最小处理单位，也可以是字或短语）组成的句子$s$，$w_{i}$表示第$i$个词，其概率公式可以表示为
\[ p(s)=p(w_{1})p(w_{2}|w_{1})p(w_{3}|w_{1}w_{2})\cdots p(w_{l}|w_{1}w_{2}\cdots w_{l - 1})=\prod_{i = 0}^{l}p(w_{i}|w_{i - 1}) \] （1.1）
虽然由于语言的复杂性，无法列举出所有可能出现的句子，但语言模型在统计层面对语言的规律进行了建模，从而能够进行语言的分布统计和推断。语言模型的实现方法有很多，其中最经典的是N元（N-gram）模型。

2. **统计语言模型**

![image](https://github.com/user-attachments/assets/448cbe30-1d2d-4430-85a2-d0daf5d5b2f2)


对于预测一个词$w_{i}$的分布概率的情况，随着历史长度的增加，可能的历史数目是按指数级增长的，这样无法高效地进行统计学习。N-gram模型假设当前词的分布概率只与前N - 1个词有关。以2-gram模型为例，当前词的分布概率只与前1个词有关，其概率公式可以表示为
\[ p(w_{i}|w_{i - 1})=\frac{c(w_{i - 1}w_{i})}{c(w_{i - 1})} \] （1.2）
式中，$c(w_{i - 1}w_{i})$表示2-gram词在给定文本中共现的次数。共现次数的比例近似体现了单词出现的概率。统计语言模型是基于构造统计语言模型的训练数据得到的，通过式（1.2），就能够利用训练数据进行统计语言模型的学习。

3. **神经网络语言模型**

N-gram模型存在一个问题，即对于没有在语料库中出现的N-gram词，无法对其进行概率估计。尽管平滑技术能够改善这个问题，使N-gram模型正常工作，但是N-gram模型在大规模语料库上训练建模的能力仍然有限。因此，研究人员提出使用神经网络在一个连续空间中构建语言模型。前馈神经网络语言模型学习词的分布式表示，使一个词能够使用一个向量表示，取得了优于N-gram模型的性能。之后被提出的循环神经网络（Recurrent Neural Network，RNN）语言模型能够对变长的输入建模，捕获更长的上下文信息。神经网络语言模型已经是目前语言模型的标准实现方法。

4. **预训练大模型**

预训练大模型在大量无标注的开放数据上进行预训练，学到了更好的语言表示，并对下游任务的性能提升有显著的效果。Word2Vec是最早的预训练大模型，之后研究人员提出了基于循环神经网络的预训练大模型ELMO。随着Transformer结构的提出，后续的预训练大模型基本都采用了以Transformer为基本结构的模型架构。
①本书中标注的[1]、[2]……对应的参考文献，请下载本书配套电子文档查看。 

### 1.1.2 预训练大模型的发展

随着深度学习的发展，语言模型已经成为一个非常重要的研究领域。目前的语言模型都使用深度神经网络对语言的生成概率进行建模。基于数据驱动的深度学习，语言模型在广泛的文本数据上进行预训练，并极大地促进了下游任务的性能提升。预训练大模型可以分为两种：一种是基于编码器结构的BERT，另一种是基于解码器结构的GPT。

1. **BERT**

BERT基于Transformer编码器结构，是一种强调学习表示的模型。BERT的训练目标是完成类似于完形填空的任务，模型预测的概率是句子中被掩蔽的词的分布概率。BERT是基于双向注意力机制对输入的句子进行编码的，因此不是一种自回归生成式语言模型。BERT由于具有强大的表示能力，所以掀起了NLP发展的一次浪潮。

2. **GPT**

GPT的建模形式与1.1.1节介绍的语言模型更一致。GPT使用一个标准的语言模型建模目标，即优化最大似然概率。在结构上，GPT只使用Transformer解码器。GPT使用朴素的语言模型的训练目标进行学习，无须特别设计预训练任务，需要在非常广泛的文本语料上进行预训练。

此外，预训练大模型也可以基于编码器 - 解码器结构，如Google提出的T5。T5同样是生成式语言模型，其将所有任务统一为生成任务进行学习，取得了不错的效果。

在预训练大模型的研究初期，BERT在学术界和工业界引起了一定的关注，其表现略胜于GPT。然而，随着ChatGPT的出现，情况发生了显著变化。基于解码器结构的自回归生成式语言模型GPT逐渐崭露头角，成为大模型时代的主流模型。这一现象产生的背后有许多原因，其中一个关键原因是GPT的预训练任务与结构更加易于扩展和优化。


### 1.1.3 大模型的进化

1. **更多的参数、更强大的能力：GPT-1、GPT-2、GPT-3**

GPT在2018年由OpenAI提出，其与BERT在同一时间被提出，但在早期并没有得到比BERT更多的关注。之后OpenAI不断升级GPT，使GPT不断进化，拥有越来越多的参数及越来越强大的能力。GPT-1的参数量为1.17亿个，GPT-2的参数量是GPT-1的10倍以上，达到15亿个，训练数据也从GB量级提升到TB量级。而GPT-3的参数量已达到1750亿个，同时具有更强大的能力。GPT-1为广泛的下游NLP任务提供了一个强大的语言基座，GPT-2已经能够在无监督的条件下完成多种任务。到大模型时代，GPT-3依赖海量的参数取得了惊人的表现，其性能在零样本或小样本条件下超越了GPT-2微调后的最好性能。

2. **推理能力的释放：CoT**

语言模型的规模不断扩大，除了在一系列NLP任务中取得了更好的性能，还在推理任务中有了独特的表现。已有研究发现，在大模型上，通过思维链（Chain of Thought，CoT）技术，即一种特殊的Prompt（提示）设计，在零样本条件下，只需要一句简单的“Let's think step by step”，就能在推理任务中取得显著的模型性能提升。值得注意的是，CoT能力只有在语言模型规模达到一定程度后（通常参数量在100亿个以上）才会出现。

3. **遵循人类指令：InstructGPT**

随着GPT的语言能力不断进化，语言模型的发展来到ChatGPT时代。虽然GPT-3具有强大的语言能力基础，但其本质上仍然是一个对语料进行统计建模，学习下一个词预测（Next Token Prediction，NTP）的语言模型。语言模型规模变得更大并不意味着模型本身能更好地遵循用户的意图。例如，大模型可能会生成不真实、有害或对用户无益的输出。面向真实世界的广泛任务，一个更好的语言模型应当能够遵循人类指令去完成任务。因此，OpenAI关注AI对齐（AI Alignment）问题，进一步训练模型遵循人类指令的能力。在ChatGPT正式推出之前，OpenAI就发布了InstructGPT。InstructGPT的实现与ChatGPT在技术上是基本一致的。

4. **大模型发展历程**

![image](https://github.com/user-attachments/assets/ae86eabb-dafd-4856-948a-d2fc6fa6366f)


图1-1以时间顺序展示了目前参数量在10亿个以上的部分大模型，其中标注为黄色的是公开模型参数的大模型。目前Llama 2是备受关注且被广泛使用的一种开源大模型。

### 图1-1 大模型发展历程
### 1.2 大模型的核心框架

随着Transformer在各类NLP任务中的应用取得突破性进展，现在主流的大模型研究都倾向于采用这一结构。本节将介绍大模型的核心框架Transformer及其主要模块。

### 1.2.1 Transformer

Transformer是一种用于NLP任务和其他序列到序列任务的深度学习模型。由Google的研究人员在2017年提出，被认为是迄今为止最具影响力的模型之一。Transformer的设计革命性地改变了序列建模领域，取代了传统的循环神经网络结构，能够取得更好的性能和更强大的并行化能力。

基于Transformer的编码器和解码器结构如下：在Transformer中，输入序列首先通过一个编码器进行编码。编码器由多个相同的层堆叠而成，每一层都包含一个多头自注意力机制和一个前馈神经网络。多头自注意力机制通过对输入序列的不同位置进行注意力权重计算，得到序列中每个位置的表示。前馈神经网络对每个位置的表示进行非线性变换，进一步丰富了编码后的表示。Transformer中的自注意力机制被扩展为多头自注意力机制，这使得模型可以同时关注不同的语义表示。在多头自注意力机制中，首先将输入序列通过线性变换映射到多个子空间中，其次在每个子空间中进行独立的注意力权重计算，最后通过将所有子空间中的结果合并得到最终的注意力表示。

### 图1-2 基于Transformer的编码器和解码器结构

![image](https://github.com/user-attachments/assets/012ac3c3-ee62-4bfe-8d31-399aac709d1c)


在编码器之后，Transformer使用解码器来生成目标序列。解码器的结构与编码器类似，但在多头自注意力机制之外，还引入了另一个自注意力机制，用于对已生成的目标序列进行注意力权重计算。这样做的目的是让解码器能够在生成每个位置的输出时，同时关注输入序列和已生成的目标序列，从而更好地捕捉输入和输出之间的对应关系。为了加速模型的训练，解码器中还引入了一种掩蔽自注意力（Masked Self-Attention）的技术，该技术在每个位置的注意力权重计算中屏蔽了该位置之后的信息，以确保模型只能依赖已生成的内容进行预测。

为了帮助模型更好地进行训练，Transformer还引入了残差连接和层归一化机制。残差连接允许模型在堆叠多个层时保留之前层的信息，并通过跳跃连接将其添加到当前层的输出中。层归一化机制则对每个子层的输出进行归一化操作，以加速模型的训练和收敛。

Transformer没有像循环神经网络一样的隐含状态，需要通过某种方式将序列中的位置信息引入模型。为此，Transformer使用了位置编码，将位置信息嵌入输入序列。位置编码向输入的每个位置添加一个固定的向量，这样模型就能够感知不同位置之间的相对距离和顺序关系。

Transformer目前已经被应用在多个领域中，除在NLP领域的翻译、摘要和对话等任务中表现出色以外，还被广泛应用于图像生成、语音识别和推荐系统等其他领域。Transformer在不同领域的突破证明了自注意力机制的强大和可扩展性，为进一步改进序列建模和深度学习提供了重要的思路。同时，也激发了许多变体模型的开发，如BERT、GPT等，这些变体在结构、训练方法、目标函数等方面进行了改进和优化，以适应不同的应用场景。

### 1.2.2 位置编码

在传统的Transformer中，位置编码是用固定的正弦函数和余弦函数的组合来表示不同位置的顺序关系的。这种方式虽然可以为模型提供位置信息，但存在一些限制。首先，传统位置编码无法适应不同长度的输入序列，因为其编码模式是固定的，无法动态调整。其次，传统位置编码无法处理具有周期性模式的序列。为了解决这些问题，旋转式位置编码（Rotary Position Embedding，RoPE）提供了一种动态的位置编码方式，通过旋转操作，位置编码可以随着模型的训练动态调整。具体来说，RoPE通过旋转正弦函数和余弦函数的相位来调整位置编码的模式。RoPE引入了一个可学习的参数，即旋转角度（Rotation Angle），该角度控制位置编码函数的旋转程度。通过调整旋转角度，RoPE可以在不同的训练阶段改变位置编码的模式，使其更具动态性和适应性。RoPE的旋转操作使位置编码能够在较大范围内形成周期性模式，更好地捕捉序列中的位置信息，这有助于提高模型在处理长序列时的性能和泛化能力。RoPE提供了一种灵活且自适应的方式来编码位置信息，对于提升模型在NLP、语音识别和其他序列建模任务中的表现非常有帮助。

### 1.2.3 多头自注意力机制

![image](https://github.com/user-attachments/assets/d8a5e127-4c68-479f-9729-0b1d893d3bd9)


Transformer的成功源于多头自注意力机制，多头自注意力机制允许模型在处理序列时通过计算注意力权重的方式为输入序列的不同位置赋予不同的重要性。注意力权重的计算公式为：

\[ Attention(Q, K, V) = softmax\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V \] （1.3）



式中，$Q$、$K$、$V$分别表示由输入序列不同位置的查询（Query）、键（Key）、值（Value）向量拼接组成的矩阵；$\sqrt{d_{k}}$为放缩因子。计算得到的注意力权重表示输入序列的每个位置对其他位置的关注程度，用于在生成输出时引导模型做出决策。此外，Transformer能够同时考虑输入序列的所有位置信息，而无须像循环神经网络那样依次处理每个位置。这种并行化的特性使Transformer能够更好地捕捉长距离的依赖关系，从而提高了模型的表示能力。

多头自注意力机制是Transformer的一项关键技术，用于提高模型对输入序列的表示能力。模型通过并行地应用多头自注意力机制来捕捉输入序列不同位置之间的多种关系。在传统的自注意力机制中，输入序列每个位置的查询、键和值都是通过线性变换得到的。而多头自注意力机制则引入了多组线性变换，每组线性变换被称为一个头，每个头都可以学习不同的查询、键和值的表示，从而捕捉输入序列中的不同位置信息。多头自注意力机制计算过程的描述如下：

- 将输入序列分别通过多组线性变换得到多组查询、键和值的表示。

- 对每个头，将对应的查询、键和值输入到单独的自注意力机制中，计算出注意力权重。

- 将每个头对应的注意力权重乘以对应的值，得到加权值向量。

- 将所有头对应的加权值向量拼接在一起，通过另一个线性变换得到最终的注意力表示。

这种多头并行计算的方式允许模型同时关注输入序列中的不同相关性，从而更好地捕捉长距离的依赖关系。相较于普通的单头自注意力机制，在多头自注意力机制中，每个头都可以专注于不同的特征子空间，可以提高模型的表示能力。

### 1.3 数据收集和数据处理

大模型的训练需要大量的无监督训练数据，通常需要混合不同来源的数据进行预训练。本节介绍大模型训练数据的收集和处理。

### 1.3.1 数据收集

文献[11]将训练语料分为通用数据和专业数据。通用数据，如网页数据、对话数据、图书数据等，由于具有种类多样、可访问的属性，因此被大多数大模型使用，能够用于训练大模型的基础建模能力。另外，为了提高大模型的泛化能力，也有研究将训练语料扩展到专业数据，如多语言数据、科学数据、代码数据、垂直领域数据等，以赋予大模型特定领域的任务解决能力。 

