### 第7章 多语言大模型技术介绍及其工业应用

随着Llama 2大模型的出现，围绕多语言大模型的研究在国内外社区火热展开。本章将从多语言大模型的研究现状和技术方向出发，系统地总结多语言大模型的训练资源和预处理、优化方向，以及多语言大模型现阶段典型的工业应用。


#### 7.1 多语言大模型的研究现状和技术方向

目前全世界人口使用的语言有上千种，是否有必要进行多语言研究和保护，以及让每种语言的使用者公平地享受大模型技术发展带来的红利，将是本节讨论的重点。同时我们也会帮助大家简略了解多语言大模型的发展趋势和技术方向。



##### 7.1.1 为什么进行多语言研究

随着互联网的发展，世界各地区的人都可以通过各种应用进行沟通和交流。现阶段互联网上主要使用的10种语言如图7-1所示，其中汉语和英语作为2种主要语言，其用户占互联网用户的45.3%，这2种语言也是现阶段NLP技术和大模型技术的主要研究语言。但值得注意的是，世界上共有7000多种语言[1]，除去上述2种语言，还有54.7%的用户并没有完全享受到大模型技术发展带来的红利，下面将从4个方面阐述多语言研究的必要性[2]。


1. **社会层面**

人们使用的语言决定了其能接受到的教育、获得的知识及人脉关系。尽管互联网是开放的，但是数字语言鸿沟（Digital Language Divide）还是存在于主流语言（汉语、英语和其他主流西方语言）和其他语言之间的。如图7-1所示，只有少数的语言会大量出现在互联网中，这就极大地提高了小语种使用者获取信息的门槛。

![image](https://github.com/user-attachments/assets/85011240-bac8-48eb-b3b4-db8c962d620a)


数字语言鸿沟的存在会在各个层面影响NLP技术的发展。例如，现阶段社交App（如微信）中使用的大多数语言都有常见的非正式输入形式（如缩写或俚语）。但是数字语言鸿沟的存在会让非主流语言使用者无法得到很好的键盘输入支持和拼写纠错服务[4]，同时会让NLP算法对非主流语言使用者存在偏见和歧视[5]，这也许不是语种导致的，而是不同的口音影响了算法结果[6]。

这种对非主流语言的忽视现象不仅会扩大数字语言鸿沟，还会使一些非主流语言使用者放弃使用他们的语言，转而使用技术支持好的、资源丰富的语言，从而影响语言的多样性。因此，为了让非主流语言使用者更好地享受技术发展带来的便利，降低算法对不同语言的效果差异，以及消除语言屏障，需要让大模型覆盖除英语和汉语外的更多语言。

2. **语言层面**

我们在训练大模型时，都希望训练出一个与语言无关的、更通用的大模型，但是大多数时候受限于语料资源，我们训练的大模型只擅长单语言任务，如汉语或英语任务。图7-2[7]列举了针对不同语言和语系的单语言BERT模型。单语言预训练大模型中的汉语、英语及一些高资源（High-Resource）语言并不能代表世界上的其他语言。以汉语为例，它属于汉藏语系（Sino-Tibetan），是一种弱词形变化（Morphologically Poor）语言，更注重在句法上表示语意，即注重词语的顺序（如注重主谓宾顺序），需要利用不同的词而不是只通过词语的变化来表示时态、单/复数和性别[8]。例如，曼尼普尔语（Meitei）中使用置于句末的te/de表示否定，thak - ke翻译成“我喝了”，而thak - de翻译成“我没有喝”[9]。

除了以上词法和句法的区别，我们还可以从一个更全面的视角观察不同语言的不同特征。《世界语言结构地图集》（World Atlas of Language Structure）[10]归类了192种特征，如主谓宾（SVO）顺序就是其中之一。其中，每种特征都包括多种类别，如主谓宾顺序这个特征就可以有SOV、SVO、VSO等不同的类别。研究发现，48%的特征只出现在低资源（Low-Resource）语言中[11]。我们在训练或微调大模型时如果没有使用这些特征，就很可能错过一些对大模型泛化有价值的信息。

以训练更好的多语言大模型为目标，可以帮助我们更好地了解世界上各种语言的关系[12]，反过来它也能帮助大模型更好地利用语言学上的特征。你可以利用你在非母语语言方面的知识，探索其他语言与你的母语在不同方面的区别，如复合（Compounding）、派生（Derivation）、重叠（Reduplication）现象的使用。

![image](https://github.com/user-attachments/assets/3e4c64a7-9f20-4098-85f1-babb3f4bd5d4)


3. **文化和道德规范层面**

我们用来训练大模型的数据并不仅仅反映对应语言的特点，从模型的回答中也会流露出文化认同和常识。但是不同的文化有不同的常识，如酒文化不会出现在阿拉伯国家。一些较小的大模型，如13B或7B的大模型，如果只在一种语言，如汉语上进行对话微调，那么其会在汉语的对话上给出合理的回答，但是对于其他语言则会出现大模型常见的幻觉现象，给出不符合事实或违反道德规范的答案。

现在大模型会被用于各种不同的复杂生成任务，而不是简单的分类任务，所以如何利用多语言资源，训练符合世界所有语言使用者文化和道德规范的大模型，促进Responsible AI（RAI）在不同语言上的发展，已经成为一个重要的课题。

4. **模型层面**

当我们训练大模型时，大模型会对训练语料中占比较大的语言产生归纳偏置（Inductive Bias），即使我们没有利用神经网络显式的编码语言信息，仅利用N-gram 组构建大模型。研究表明，模型效果在具有丰富词形变化的语言上也是显著下降的[13]。

Transformer - based模型还会忽略具有丰富词形变化的语言的复杂性[14]：子词分词器（Subword Tokenization）在具有叠词的语言上表现不佳[15]；BPE算法无法很好地对齐词形信息[16]；大模型虽然在单语言和跨语言任务上已经显示出了零样本学习能力，但是在低资源语言或与主流训练语言类别距离较远的语言上，文本生成和分类性能均有下降[17]；与进行过任务微调的大模型相比，其性能还有很大的差距[18]。

以上问题对如何利用词语、句子信息构建多语言大模型提出了挑战。近年来，许多学者也开始关注多语言大模型在低资源语言上的跨语言学习能力，以及多语言组合提示在大模型上的ICL能力等。

##### 7.1.2 多语言在NLP社区的发展

讨论完研究多语言的必要性，本节我们来看一下现阶段多语言在NLP社区的发展。



1. **多语言发展趋势**

现在有很多研究机构致力于多语言的研究，既包括像汉语、日语、土耳其语和印度语这样覆盖庞大人口的语言，也包括像爱尔兰语等只覆盖少量人口的语言。近些年也出现了一些NLP社区专门研究关注度不足（Under - Represented）的语言或语系，更多NLP社区专注于区域性语言研究。例如，研究非洲地区语言的Masakhane，研究原始美洲语言的AmericasNLP，以及研究印尼地区语言的IndoNLP。同时还有专门为非英语语言研究举办的长期研讨会及活动，如中国中文信息学会每年组织的定期会议（如CCL、ACL）为语言类型学设立的兴趣小组（如SIGTYP、AfricaNLP、ArabicNLP和ComputeEL）。



与此同时，有一些社区关注更广泛的语种和工作，如ML Collective和Big Science。Big Science是致力于服务多语言AI的社区，其发布了BLOOM[19]，并且在BLOOM和mT5[20]这两个多语言大模型上进行多任务提示微调，构建了跨语言能力强的BLOOMZ和mT0[18]。



为了突出多语言的重要性，ACL不仅设立了SIGTYP，还在2022年设立了Special Theme Track，旨在通过以下努力让科学论文被更多人接触到。

- 将ACL选集（ACL Anthology）翻译成60种语言。

- 将全体会议用10种语言配音并且增加字幕。

- 将一个全面且标准的NLP术语集翻译成60种语言。



这些资源和术语表可以让不同地区的人使用自己的语言来讨论NLP技术。7.2节、7.3节将会详细介绍促进多语言技术发展的数据集和模型。



2. **多语言发展挑战**

下面介绍2个典型的多语言发展挑战。

（1）**多语言诅咒**。

为什么现在的多语言大模型最多只能覆盖100多种语言？除了资源原因，还有一个原因是多语言诅咒（Curse of Multilinguality）[21]。和在多个任务上训练大模型一样，使用越多的语言训练大模型，由于受限于模型的容量（几百兆比特），大模型会越难学到每种语言的表征信息。多语言大模型的出现打破了这个瓶颈，动辄10B以上的参数规模可以让大模型更好地学习每种语言的表征信息[22]。

（2）**低资源问题**。

多语言大模型发展的一个首要问题就是可使用的语料资源呈现长尾分布，我们常说的高资源语料极大地偏向于以英语为代表的印欧语系和汉语、日语、韩语等，这些头部语言无论是在标注数据（Labeled Data）中，还是在无标注数据（Unlabeled Data）中，数量都是巨大的。我们按照标注数据和无标注数据2个维度，将全球语言分成了0~5的6个类别[11]，分别表示大模型可以利用语言的难度，0表示最难。如图7-3所示，其中虚线包围的不同颜色区域的面积代表包含语言的多少，其中颜色由深到浅分别代表使用这种语言的人数从多到少。

![image](https://github.com/user-attachments/assets/284879a4-9ef9-4e3a-bb29-2642a5387298)


- **类别0（The Left - Behinds）** 。

NLP技术一直以来都忽视了类别0的语言（见表7-1中的类别0）。因为语料资源极少，所以这类语言将逐渐成为历史，很难将它们与数智化挂钩。就算是利用无监督学习（Unsupervised Learning）的方法，也只会让它们越来越差，因为它们基本上没有可以使用的无标注数据。

![image](https://github.com/user-attachments/assets/bb30a441-fa21-4345-b342-71c89e170c13)


- **类别1（The Scraping - Bys）** 。

类别1的语言（如格陵兰语）中有一些具备一定数量级的无标注数据，使它们有可能在数年后在多语言研究中受到更多研究人员的关注。但是这需要研究人员有组织、持续地推动涉及这类语言的任务，让更多的人关注它们，并且愿意为它们收集更多的标注数据。这类语言现阶段标注数据的数量几乎为0。

- **类别2（The Hopefuls）** 。

类别2的语言（如爱尔兰语）在NLP领域正经历黎明前的黑暗，努力前进。这类语言积累了一些标注数据（数量较少），这意味着有一批研究人员正在努力对这类语言进行数智化转型，预计他们在未来几年会制作出一些很有前景的NLP工具服务于这类语言。

- **类别3（The Rising Stars）** 。

从类别3往后基本就是我们日常所说的高资源语言了。无监督学习方法极大地扩大了这类语言（如印尼语）在NLP领域的影响。因为这类语言在互联网上有更多的使用者，所以很多NLP社区研究人员致力于这类语言的研究，但是他们的研究也会因标注数据不足而受到影响。这些研究人员应该借助大模型的预训练和参数高效微调技术[23]来弥补标注数据不足的影响。

- **类别4（The Underdogs）** 。

类别4的语言（如越南语）在NLP领域就像火种一样，发展潜力巨大。它们积累了大量的无标注数据，在标注数据方面，与类别5相比只少了一个数量级。很多经验丰富的社区研究人员致力于这类语言的研究。这类语言很有潜力变成类别5的语言，并且是能体验到数智化优越性的一类语言。

- **类别5（The Winners）** 。

类别5的语言（如汉语、英语）在NLP领域的发展是非常迅猛的，并且一直处于领先地位，其研究的时间也比前几个类别的语言要长。因为这类语言在网络上有着主导地位，有大量的企业及政府机构对这类语言在NLP领域的资源和技术发展进行投资，它们是绝对的高资源语言。这类语言的使用者享受着NLP领域最先进的成果及技术发展带来的红利。



值得一提的是，我国是一个统一的多民族国家，具有民族多、语言多、文字多的特色。除汉族外，我国有占全国总人口8%的55个少数民族[24]。这些少数民族，除回族大多使用汉语言文字外，其他少数民族（如蒙古族、藏族、维吾尔族、哈萨克族、朝鲜族等）不同程度地使用本民族文字。国家也决定在“两会”等重大会议上提供蒙古族、藏族、维吾尔族、哈萨克族、朝鲜族、彝族、壮族7种少数民族语言文字文件和同声传译服务，充分体现了各民族平等的民族政策。同时中国中文信息学会组织的CCL作为国内最大的中文社区，鼓励中国少数民族语言相关的计算语言学方面的原创研究和应用论文发表[25]。另外，根据国际化计算机处理技术发展要求，我国对藏文、蒙文、满文、傣文、佉卢字、西夏文、八思巴字、傈僳文等古文字创建了编码系统，进而为少数民族古文字及其文化信息交流提供了更为广阔的学术空间。我国为女真字、契丹小字、契丹大字、突厥文、水书、东巴文、傈僳文等设定的字符方案获得了国际标准化组织的高度评价和认同，抢占了该领域的学术话语权。



##### 7.1.3 多语言模型技术方向

7.1.2节中提到语言的分类决定了不同语言的研究方向，如类别3和类别4的语言，由于缺乏足够的标注数据，因此可以利用无监督学习方法来弥补。近年来，随着大模型的发展，相关研究主要通过预训练技术学习无标注数据中的语言特征，实现目标语言的零样本或小样本学习。目前学术界也有相关研究成果的发表，主要集中在东南亚的印尼语[26]、越南语[27]和泰语[28]。



当然仅借助无标注数据来提升类别3和类别4的语言任务的性能是不够的，一个很自然的想法是借助高资源语言中的语言特征向低资源语言迁移，即跨语言迁移学习（Cross Lingual Transfer Learning）和多语言学习（Multilingual Learning），前者是指从1个源语言迁移到1个目标语言（one - to - one），后者是指从多个源语言迁移到1个目标语言，在BLOOM/Llama类开源大模型出现以后，后者逐渐成为研究的主流。但是，在跨语言迁移学习的过程中，语言间的相似性极大地制约了迁移的性能。以英语和西班牙语在机器翻译上的迁移为例，因为两种语言很接近，在单词上有近50%的重叠度，所以小样本迁移效果和翻译效果很好。但是汉语和英语由于token上的差异及其他特征差异较大，因此迁移效果不佳，甚至可能带来负向迁移（Negative Transfer）和灾难性遗忘（Catastrophic Forgetting）的问题。如何选择有效的高资源语言作为源语言，可以参考文献[8]、[29]的相关研究。



2023年以来，随着大模型的发展，提示工程成为一个火热的研究课题，其大致分为两个技术方向。

（1）**构造Prompt，进行ICL**。这个技术方向利用Prompt作为输入，但是不对模型参数进行微调，在多语言侧大致分为跨语言提示（Cross Lingual Prompt）和跨语言思维链提示（Cross Lingual Chain - of - Thought Prompt）。两者都利用英语作为提示的主要语言，但是测试样本使用目标语言，这类方法已被证明可以取得更好的效果，并且和将测试样本翻译成英文的效果相似[30]。

（2）**利用机器翻译的多语言提示（Multilingual Prompt）任务进行模型的参数微调**。实验效果证明，使用英语作为提示的主要语言，在多语言任务下进行微调，能在英语或非英语任务集的零样本学习任务上获得State - of - the - Art（SOTA）效果，同时使用机器翻译的多语言提示进行微调，在一些语言上也比人工翻译的提示微调结果好[18]。


#### 7.2 多语言大模型的预训练资源和评测任务

数据资源在如今这个大模型时代与石油和煤炭在工业时代一样重要。本节将总结多语言大模型训练所使用的主流语料，以及如何对这些语料进行加工、提炼，以便更好地帮助研究人员提升多语言大模型的效果。



##### 7.2.1 多语言大模型的预训练资源介绍

1. **预训练资源**

大模型需要海量的训练语料才可以学习到更全面的知识和内容，所以现在越来越多的开源训练语料被用来训练大模型。本节按照训练语料的内容类型，简要地将目前被广泛使用的语料资源归类为书籍资源（Books）、网页资源（CommonCrawl）、Reddit资源（Reddit Links）、维基百科（Wikipedia）、代码（Codes）和其他（Other）[31]，如表7-2所示。



|语料资源|数据大小|数据源头|更新时间|
| ---- | ---- | ---- | ---- |
|BookCorpus| - |Books|2015-12|
|Project Gutenberg| - |Books|2021
