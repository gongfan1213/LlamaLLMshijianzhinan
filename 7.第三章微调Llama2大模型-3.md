当预训练模型的参数很少时，P-Tuning v1的效果很差，只有当参数达到100亿个时，它才勉强达到Fine-Tuning的效果。而P-Tuning v2虽然在一些任务上可以达到与Fine-Tuning一样甚至更好的效果，但灾难性遗忘的情况比较严重。

### 3.4 模型评估、测试和模型优化

模型评估、测试和模型优化是深度学习中至关重要的步骤，可以帮助我们了解模型的性能、发现问题并不断提高模型的质量。本节以Llama 2大模型为例，深入探讨这两个关键步骤。



### 3.4.1 模型评估、测试

模型评估、测试是开发大模型的关键步骤，用于评估大模型的性能并确保其在特定任务中的有效性和准确性。

在模型评估、测试过程中，我们通过将大模型的输出与参考标准或基准数据集进行比较，来评估其质量和准确性。模型评估、测试的具体指标取决于模型设计的特定任务。

模型评估、测试的目的是确保大模型在实际应用中的有效性。通过评估、测试模型的性能，我们可以识别和解决模型中的问题，并进行必要的调整和改进，以提高模型的性能和泛化能力。总之，模型评估、测试是开发大模型不可或缺的步骤，通过比较模型的输出与参考标准来评估模型的质量，并使用适当的指标来衡量模型的性能。这些步骤可以帮助我们改进和优化模型，以确保其在特定任务中的有效性和准确性。

对于语言建模任务，困惑度是常用的评估模型质量的指标。此外，我们还可以从不同任务场景的角度来评估微调后模型的性能，以下角度可以作为参考。

- **基础能力**：如给定一个话题、一个写作任务，要求据此创作一段文字，根据模型生成的结果（内容是否丰富、用词是否恰当、语句是否通顺等）来评估微调后模型的性能。

- **语义理解与抽取**：如让模型从一篇文章、一段话中抽取出指定的知识（出现的人物、地点和事件等）。 

- **逻辑推理**：如向模型提一个比较简单的逻辑推理问题，观察模型是否能给出正确的答案。 

- **代码生成**：如让模型写一个快速排序等简单算法的代码。 

- **数学计算**：如加、减、乘、除的计算测试。

模型评估、测试是验证大模型在未参与训练或验证的新独立数据集上的表现的过程。通过模型测试，我们可以评估模型的性能，并确定其在实际应用场景中的解决目标任务的有效性和准确性。

为了确保模型能够处理各种输入变化并在未知数据上表现良好，使用多样性和有代表性的测试集非常重要。测试集应该包含各种不同类型的样本，涵盖模型可能会遇到的各种情况和场景（可以参考开源的BLUB项目中的测试集，包含通用知识、语言理解、创作能力、逻辑推理、代码编程、工作技能、使用工具、人格特征这八个大类别，其中每个大类别下又分别包含很多小类别）。这样可以更全面地评估模型的性能，并发现模型在特定领域或特定类型的输入上的潜在问题。

为了有效评估、测试模型，可以采取以下评估、测试策略，以便进行准确和全面的模型评估、测试。

- **数据集划分**：将数据集划分为训练集、验证集和测试集。训练集用于进行模型训练，验证集用于调整模型的参数和结构，测试集用于最终评估模型的性能。要确保数据集的划分是随机且有代表性的，以避免样本偏差对评估结果产生影响。

- **评估指标**：选择适当的评估指标来评估模型的性能。根据任务的特点，可以使用准确率、精确率、召回率、F1值等指标来评估模型的性能。此外，还可以考虑其他领域特定的指标，以便更好地评估模型在实际应用中的效果。 

- **多次测试**：进行多次测试以获取更可靠的结果。在每次测试时，可以随机选择不同的样本或使用交叉验证方法来获得更全面的评估结果。进行多次测试可以减小随机性对评估结果的影响，并获得更稳定的评估指标。 

- **模型调整**：根据测试结果调整模型的参数和结构。通过分析测试结果，可以发现模型的弱点和改进空间，并进行相应的调整和优化。可以尝试采用不同的超参数设置、模型结构变化或正则化技术，以提高模型的性能和泛化能力。



### 3.4.2 模型优化

针对模型评估、测试中出现的问题，可以采取一些有针对性的方法。例如，当模型生成的文本不遵循原文（Faithfulness）或不符合事实（Factualness）时，我们可以认为模型出现了幻觉。




在训练数据层面上，在训练数据收集过程中可能会收集到虚假信息，从而导致模型记忆了错误的知识，过多的重复信息也可能导致模型的知识记忆出现bias，从而出现幻觉。即使有高质量的训练数据，模型仍然可能出现幻觉。

在模型层面上，有研究表明，解码算法如果使用不确定性较高的采样算法（如top-P），则会诱导模型出现严重的幻觉。甚至可以故意在解码算法中加入一些随机性，进一步让模型“胡编乱造”。参数知识的问题也会导致模型在预训练阶段记忆错误的知识，从而出现严重的幻觉。

为了减轻模型的幻觉，可以从以下几个方面入手。在训练数据层面上，可以人工标注一批高质量的训练数据，并剔除可能导致出现幻觉的训练数据。在评估、测试数据层面上，可以构建细粒度的幻觉评估Benchmark，用于分析幻觉的严重程度和原因。在模型层面上，可以尝试进行强化学习（如RLHF），将减轻幻觉的指标作为强化学习的Reward函数，从而减轻模型的幻觉。可以在解码时减小模型的生成随机性，进行多任务学习，通过设计合适的额外任务，达到减轻模型幻觉的效果。在后处理方面，可以设计一个小模型专门用于修复幻觉错误。



### 3.5 模型保存、模型部署和推理加速

模型保存、模型部署和推理加速是将深度学习模型（如Llama 2）投入实际应用的关键步骤。在这个主题下，本节将探讨如何有效地保存微调后的Llama 2大模型并将其部署到实际应用中，以及推理加速的相关内容。



### 3.5.1 模型保存

对于用LoRA微调训练后的模型，可以参考下面的代码保存：

```python
peft_model_id = f"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}"
model.save_pretrained(peft_model_id)
```

调用save_pretrained()函数保存模型，保存后的结果如下：

```
├── [ 451]  adapter_config.json
├── [ 81K]  adapter_model.bin
└── [ 129]  README.md
```

这里只保存经过训练的增量模型权重。其中，adapter_config.json为LoRA配置文件；adapter_model.bin为LoRA权重文件。

如果想要得到完整的模型文件，则需要将用LoRA微调训练后的增量模型权重和Llama2-base模型进行合并。

### 3.5.2 模型部署

1. **命令行原生Transformers推理接口**


本节提供了命令行的方式，使用原生Transformers推理接口进行推理。下面以加载Llama2-7B-Chat模型为例说明启动方式：

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# 从预训练模型路径加载CausalLM模型，并设置设备映射、张量数据类型和使用8位加载模型
model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-chat-hf', device_map='auto', torch_dtype=torch.float16, load_in_8bit=True)
# 将模型设置为评估模式
model = model.eval()
# 从预训练模型路径加载分词器，并禁用快速分词模式
tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf', use_fast=False)
# 将分词器的pad_token设置为eos_token
tokenizer.pad_token = tokenizer.eos_token
input_text = '介绍一下中国'
# 将输入文本转换为模型输入张量，并添加特殊令牌，将张量移动到GPU上
input_ids = tokenizer([f'<s>Human: {input_text}\n</s><s>Assistant: '], return_tensors="pt", add_special_tokens=False).input_ids.to('cuda')
# 定义生成文本的参数
generate_input = {
    "input_ids":input_ids,
    "max_new_tokens":512,
    "do_sample":True,
    "top_k":50,
    "top_p":0.95,
    "temperature":0.3,
    "repetition_penalty":1.3,
    "eos_token_id":tokenizer.eos_token_id,
    "bos_token_id":tokenizer.bos_token_id,
    "pad_token_id":tokenizer.pad_token_id
}
# 使用模型生成文本
generate_ids = model.generate(**generate_input)
# 将生成的文本张量解码为文本字符串
text = tokenizer.decode(generate_ids[0])
print(text)
```

首先，导入PyTorch库和Transformers库，使用AutoModelForCausalLM.from_pretrained()方法加载预训练的Llama 2大模型，使用model.eval()方法将模型设置为评估模式，使用AutoTokenizer.from_pretrained()方法加载预训练的tokenizer，并将分词器的pad_token设置为eos_token。

其次，指定输入文本input_text，使用tokenizer()方法将输入文本编码为input_ids，使用“Human: ”“Assistant: ”将输入文本包装在Human和Assistant的对话中。

再次，使用model.generate()方法生成聊天回复，将input_ids作为输入，并指定一些生成参数，如max_new_tokens、do_sample、top_k、top_p、temperature、repetition_penalty、eos_token_id、bos_token_id和pad_token_id。

最后，使用tokenizer.decode()方法将生成的文本张量解码为文本字符串，并将其打印到控制台。

这段代码使用了Llama 2大模型进行聊天，可以根据需要进行修改和扩展。

2. **基于Gradio搭建问答平台**

Gradio是一个开源的Python库，用于构建交互式的机器学习界面，使用户可以轻松地与训练好的机器学习模型进行交互。Gradio的目标是让机器学习模型更容易部署和共享，而不需要进行深度编程。

Gradio的主要特点和用途如下。

- **交互式界面构建**：Gradio允许用户使用简单的Python代码构建具有用户界面的机器学习模型，可以创建文本框、图像上传框、滑块等元素，让用户与机器学习模型进行交互。 

- **快速部署**：使用Gradio可以快速将机器学习模型部署为Web应用，而不需要深度的Web开发知识。这使得机器学习模型的展示和共享变得非常容易。 

- **多种输入、输出类型支持**：Gradio支持多种输入数据类型，包括文本、图像、音频等，还可以输出不同类型的结果，如文本、图像等。 

- **支持多种机器学习框架**：Gradio可以与多种常见的机器学习框架（如TensorFlow、PyTorch、scikit-learn等）无缝集成，因此用户可以使用自己最熟悉的框架来构建模型。 

- **适用于教育和研究**：Gradio不仅适用于将模型部署为应用程序，还适用于教育和研究。用户可以使用Gradio构建交互式的演示，以帮助学生或同事理解机器学习模型的工作原理。 

总之，Gradio是一个强大的工具，用于构建交互式的机器学习界面，使用户更容易与机器学习模型进行交互。基于Gradio搭建的问答平台实现了流式的输出，将下面的代码复制到控制台运行（以下代码以Llama2-7B-Chat模型为例，不同模型只需修改一下代码里的模型名称）：

```python
python examples/chat_gradio.py --model_name_or_path meta-llama/Llama2-7b-chat
```

```python
import gradio as gr
import time
from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer
from threading import Thread
import torch,sys,os
import json
import pandas
import argparse

with gr.Blocks() as demo:
    gr.Markdown("""<h1><center>智能助手</center></h1>""")
    chatbot = gr.Chatbot()
    msg = gr.Textbox()
    state = gr.State()
    with gr.Row():
        clear = gr.Button("新话题")
        re_generate = gr.Button("重新回答")
        sent_bt = gr.Button("发送")
    with gr.Accordion("生成参数", open=False):
        slider_temp = gr.Slider(minimum=0, maximum=1, label="temperature", value=0.3)
        slider_top_p = gr.Slider(minimum=0.5, maximum=1, label="top_p", value=0.95)
        slider_context_times = gr.Slider(minimum=0, maximum=5, label="上文轮次", value=0, step=2.0)

    # 定义用户输入的回调函数
    def user(user_message, history): return "", history + [[user_message, None]]

    # 如果上一条历史记录的回答不为空，则将其设置为空
    def bot(history,temperature,top_p,slider_context_times):if pandas.isnull(history[-1][1])==False:
        history[-1][1] = Noneyield history
        slider_context_times = int(slider_context_times)
        history_true = history[1:-1]
        prompt = '#' 构造生成文本的prompt
        if slider_context_times>0:
            prompt += '\n'.join([("<s>Human: "+one_chat[0].replace('<br>','\n')+'\n</s>' if one_chat[0] else '') +"<s>Assistant: "+one_chat[1].replace('<br>','\n')+'\n</s>'  for one_chat in history_true[-slider_context_times:] ])
        prompt +=  "<s>Human: "+history[-1][0].replace('<br>','\n')+'\n</s><s>Assistant:'
        # 将prompt转换为模型输入张量，并添加特殊令牌，将张量移动到GPU上
        input_ids = tokenizer([prompt], return_tensors="pt",add_special_tokens=False).input_ids[:, -512:].to('cuda')

        # 定义生成文本的参数
        generate_input = {
            "input_ids":input_ids,
            "max_new_tokens":512,
            "do_sample":True,
            "top_k":50,
            "top_p":top_P,
            "temperature":temperature,
            "repetition_penalty":1.3,
            "streamer":streamer,
            "eos_token_id":tokenizer.eos_token_id,
            "bos_token_id":tokenizer.bos_token_id,
            "pad_token_id":tokenizer.pad_token_id
        }
        # 创建线程，使用模型生成文本
        thread = Thread(target=model.generate, kwargs=generate_input)
        thread.start()
        start_time = time.time()
        bot_message = ''
        print('Human:',history[-1][0])
        print('Assistant: ',end='',flush=True)

        # 以文本流的形式生成文本
        for new_text in streamer:
            print(new_text,end='',flush=True)
            if len(new_text)==0:
                continueif new_text!='</s>':
                bot_message+=new_text
            if 'Human:' in bot_message:
                bot_message = bot_message.split('Human:')[0]
            history[-1][1] = bot_message
            yield history
        end_time =time.time()
        print()
        print('生成耗时:',end_time-start_time,'文本长度:',len(bot_message),'字耗时:',(end_time-start_time)/len(bot_message))

    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(
        bot,
        [chatbot,slider_temp,slider_top_p,slider_context_times], chatbot
    )
    sent_bt.click(user, [msg, chatbot], [msg, chatbot], queue=False).then(
        bot,
        [chatbot,slider_temp,slider_top_p,slider_context_times], chatbot
    )
    re_generate.click( bot,
    [chatbot,slider_temp,slider_top_p,slider_context_times], chatbot )
    clear.click(lambda: [], None, chatbot, queue=False)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name_or_path", type=str, help='model name or path')
    parser.add_argument("--is_4bit", action='store_true', help='use 4bit model')
    args = parser.parse_args()

    # 从预训练模型路径加载分词器，并禁用快速分词模式，将pad_token设置为eos_token
    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path,use_fast=False)
    tokenizer.pad_token = tokenizer.eos_token

    # 如果不使用4位模型，则从预训练模型路径加载AutoGPTQForCausalLM模型，并设置设备映射、张量数据类型和使用8位加载模型。如果使用4位模型，则从预训练模型路径加载AutoGPTQForCausalLM模型
    if args.is_4bit==False:
        model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path,device_map='auto',torch_dtype=torch.float16,load_in_8bit=True)
        model.eval()
    else:
        from auto_gptq import AutoGPTQForCausalLM
        model = AutoGPTQForCausalLM.from_quantized(args.model_name_or_path,
        low_cpu_mem_usage=True, device="cuda:0",
        use_triton=False,inject_fused_attention=False,inject_fused_mlp=False)

    # 创建文本流

streamer = TextIteratorStreamer(tokenizer,skip_prompt=True)
if torch.__version__ >= "2" and sys.platform != "win32":
    model = torch.compile(model)
```
# 启动Gradio应用程序

```python
demo.queue().launch(share=False, debug=True,server_name="0.0.0.0")
```

### 3. 基于Docker部署问答接口

部署问答接口只需在Docker环境中执行chat_gradio.py程序，系统需要准备的环境为docker:24.0.2。

- **第一步，准备Docker镜像**：

```bash
git clone https://github.com/FlagAlpha/Llama2-Chinese.git
cd Llama2-Chinese
sudo docker build -f docker/Dockerfile -t FlagAlpha/llama2-chinese-7b:gradio .
```

- **第二步，通过docker-compose启动chat_gradio**：

```bash
cd Llama2-Chinese/docker
docker-compose up -d --build
```

### 3.5.3 推理加速

我们在讨论推理加速时，实际上是在讨论如何让计算机更快地理解和处理自然语言（如文本）。这对于很多应用都非常重要，如智能助手、自动翻译、搜索引擎等。

推理是一个计算机模型根据已有的知识和数据做出决策或回答问题的过程。有时这个过程非常耗时，如在处理大量的文本或复杂的模型时。

推理加速的目的就是尽量减少计算机进行推理所需的时间，使其更快地做出决策或回答问题。这对于提升用户体验和快速处理大量数据非常重要。

前文提到的一些方法和工具可以帮助计算机实现推理加速。通过使用一些技巧，如使用更快的计算方式、更高效地存储数据、并行处理等，可提高推理速度。

简而言之，就像把一本厚重的书变成电子书，让你可以更快地查找信息，或者让你的智能手机更快地回答你的问题。总之，推理加速就是为了让计算机更聪明，从而更快地理解和处理人们的需求。



vllm同样是GPU推理的方案。相较于FasterTransformer，vllm更加简单易用。vllm的特点如下。

- **简单易用**：与其他推理框架（如FasterTransformer）相比，vllm更加简单易用，不需要额外的模型转换，使模型部署和使用变得非常方便。这对于模型开发人员和研究人员来说，节省了宝贵的时间和资源。 

- **支持fp16推理**：vllm支持fp16（半精度浮点数）推理，这可以显著地提高推理速度，同时减少GPU内存的使用。这对于大模型和大型数据集的处理非常有益。 

- **推理速度快**：vllm的推理速度非常快，这得益于其高效的实现和优化。这意味着在实际应用中，vllm可以在短时间内处理大量的NLP任务，从而提高系统的响应速度。 

- **高效的kv缓存机制**：vllm使用了高效的kv（键值）缓存机制，可以加速模型的推理。这种缓存机制有效地减少了计算时间，特别是对于需要多次查询相同的kv的任务而言非常高效。 

- **连续的批处理请求推理**：vllm支持连续的批处理请求推理。这意味着可以一次性处理多个输入，进一步提高了系统的吞吐量。这在实时应用（如聊天机器人或在线翻译系统）中尤其有用。 

- **优化的cuda算子**：vllm使用了优化的cuda算子，以充分利用GPU的性能。这些算子经过精心设计，可以确保推理过程的高效性和稳定性。 

- **支持分布式推理**：vllm支持分布式推理。这意味着可以在多个GPU或多台机器上同时进行推理，以应对大规模任务和高并发需求。这种可扩展性使得vllm成为处理大型数据集和复杂应用的理想选择。



**第一步，安装vllm**：

```bash
git clone https://github.com/vllm-project/vllm
cd vllm && python setup.py install
```

**第二步，启动测试server**：

- **单卡推理**：

```bash
bash single_gpus_api_server.sh
```

single_gpus_api_server.sh如下：

```bash
CUDA_VISIBLE_DEVICES=0 python api_server.py \
--model "/mnt/data_online/models/llama/models--meta-llama--Llama2-7b-chat-hf" \
--port 8090
```

- **多卡推理**：

```bash
# multi_gpus_api_server.sh里面的CUDA_VISIBLE_DEVICES指定了要使用的GPU卡
# tensor-parallel-size指定了GPU卡的个数
bash multi_gpus_api_server.sh
```

multi_gpus_api_server.sh如下：

```bash
CUDA_VISIBLE_DEVICES=0 python api_server.py \
--model "/mnt/data_online/models/llama/models--meta-llama--Llama2-7b-chat-hf" \
--port 8090
```

其中，启动模型API服务的接口参考代码api_server.py如下：

```python
import argparse
import json
from typing import AsyncGenerator

from fastapi import BackgroundTasks, FastAPI, Request
from fastapi.responses import JSONResponse, Response, StreamingResponse
import uvicorn

from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine
from vllm.sampling_params import SamplingParams
from vllm.utils import random_uuid

# 导入所需的库和类，并创建FastAPI
TIMEOUT_KEEP_ALIVE = 5  # seconds.
TIMEOUT_TO_PREVENT_DEADLOCK = 1  # seconds.
app = FastAPI()

# 定义生成文本的接口，使用装饰器@app.post将其绑定到HTTP POST请求上
# 接口接收一个JSON对象作为请求体，包含以下字段
# prompt: 生成文本的提示
# stream: 是否流式传输结果
# 从请求体中获取这些字段，并使用它们调用engine.generate()方法生成文本
@app.post("/generate")
async def generate(request: Request) -> Response:
    """Generate completion for the request.
    The request should be a JSON object with the following fields:
    - prompt: the prompt to use for the generation.
    - stream: whether to stream the results or not.
    - other fields: the sampling parameters (See `SamplingParams`
      for details).
    """
    request_dict = await request.json()
    prompt = request_dict.pop("prompt")
    stream = request_dict.pop("stream", False)
    sampling_params = SamplingParams(**request_dict)
    request_id = random_uuid()
    results_generator = engine.generate(prompt, sampling_params,
                                        request_id)

    # Streaming case
    # 如果stream为True，则使用异步生成器stream_results()流式传输结果。在
    # 每次生成文本时，先将生成的文本添加到text_outputs列表中，并将其作为JSON对象返
    # 回。然后使用yield将JSON对象编码为字节流，并在每个JSON对象之间添加空字符
    # '\0'。如果客户端断开连接，则调用abort_request()方法中止请求
    async def stream_results() -> AsyncGenerator[bytes, None]:
        async for request_output in results_generator:
            prompt = request_output.prompt
            text_outputs = [
                prompt + output.text for output in request_output.outputs
            ]
            ret = {"text": text_outputs}
            yield (json.dumps(ret) + "\0").encode("utf-8")

    async def abort_request() -> None:
        await engine.abort(request_id)

    if stream:
        background_tasks = BackgroundTasks()
        # Abort the request if the client disconnects.
        background_tasks.add_task(abort_request)
        return StreamingResponse(stream_results(),
                                 background=background_tasks)

    # Non-streaming case
    # 如果stream为False，则使用for循环遍历生成的文本，并将其添加到text_
    # outputs列表中。如果客户端断开连接，则调用abort_request()方法中止请求。最后将
    # text_outputs列表作为JSON对象返回
    final_output = None
    async for request_output in results_generator:
        if await request.is_disconnected():
            # Abort the request if the client disconnects.
            await engine.abort(request_id)
            return Response(status_code=499)
        final_output = request_output

    assert final_output is not None
    prompt = final_output.prompt
    text_outputs = [prompt + output.text for output in
                    final_output.outputs]
    ret = {"text": text_outputs}
    return JSONResponse(ret)

if __name__ == "__main__":
    # 解析命令行参数，创建AsyncLLMEngine对象，并使用uvicorn启动FastAPI
    parser = argparse.ArgumentParser()
    parser.add_argument("--host", type=str, default="0.0.0.0")
    parser.add_argument("--port", type=int, default=8090)
    parser = AsyncEngineArgs.add_cli_args(parser)
    args = parser.parse_args()

    engine_args = AsyncEngineArgs.from_cli_args(args)
    engine = AsyncLLMEngine.from_engine_args(engine_args)

    uvicorn.run(app,
                host=args.host,
                port=args.port,
                log_level="debug",
                timeout_keep_alive=TIMEOUT_KEEP_ALIVE)
```

等待API服务启动之后，就可以进行推理测试，推理参考脚本如下：

```python
# coding=utf-8import json
import time
import urllib.request
import sys

# 定义一个函数gen_prompt()，用于生成输入文本的提示。将输入文本包装在"<s>Human:
# "和"\n</s><s>Assistant: "之间
def gen_prompt(input_text):
    prompt = "<s>Human: "+input_text+"\n</s><s>Assistant: "return prompt

# 定义一个函数test_api_server()，用于向API服务器发送请求并获取生成的文本
def test_api_server(input_text):
    # 首先设置请求头的Content-Type为application/json。其次使用gen_
    # prompt()函数生成输入文本的提示
    header = {'Content-Type': 'application/json'}
    prompt = gen_prompt(input_text.strip())

    # 构造请求的数据，包括prompt和其他采样参数
    data = {
        "prompt": prompt,
        "stream" : False,
        "n" : 1,
        "best_of": 1,
        "presence_penalty": 0.0,
        "frequency_penalty": 0.2,
        "temperature": 0.3,
        "top_p" : 0.95,
        "top_k": 50,
        "use_beam_search": False,
        "stop": [],
        "ignore_eos" :False,
        "max_tokens": 2048,
        "logprobs": None
    }

    # 创建一个urllib.request.Request对象，指定请求的URL、请求头和请求数
    # 据。尝试发送请求并获取响应，将响应解码为json格式，并打印请求数据和生成结果
    request = urllib.request.Request(
        url='http://127.0.0.1:8090/generate',
        headers=header,
        data=json.dumps(data).encode('utf-8')
    )

    result = Nonetry:
        response = urllib.request.urlopen(request, timeout=300)
        res = response.read().decode('utf-8')
        result = json.loads(res)
        print(json.dumps(data, ensure_ascii=False, indent=2))
        print(json.dumps(result, ensure_ascii=False, indent=2))

    except Exception as e:
        print(e)

    return result

if __name__ == "__main__":
    test_api_server("如何去北京?")
    test_api_server("肚子疼怎么办?")
    test_api_server("帮我写一个请假条")
``` 
