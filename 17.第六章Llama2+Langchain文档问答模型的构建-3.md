强大的AI模型。我们将使用它来进行嵌入模型、对话模型等操作。

（5）tiktoken：tiktoken是OpenAI的一个库，它允许用户计算文本字符串中的标记数，而无须进行API调用。

（6）python-dotenv：python-dotenv允许用户在env文件中指定环境变量，这对于管理密钥和其他配置值非常有用。



至此，已经搭建好了Python环境，可以开始使用LangChain和语言模型应用程序了。

### 6.4 Llama 2+LangChain文档问答模型评估

本节将详细介绍如何从零开始创建自己的文档问答模型，使用Llama 2和LangChain这两个开源库来实现无缝集成，详细步骤如下。

（1）设置虚拟环境和创建文件结构。

（2）在本地计算机上使用Llama 2。

（3）将Llama 2与LangChain集成，并自定义提示模板。

（4）文档检索和答案生成。

#### 6.4.1 设置虚拟环境和创建文件结构

设置虚拟环境可以为应用程序提供一个受控且隔离的环境，确保其依赖项与系统范围内的其他软件包分离。这种方法简化了依赖项管理，并且有助于在不同环境之间保持一致性。

为了给应用程序设置虚拟环境，我们将在存储库中提供一个pip文件。要按照如图6 - 5所示的文件结构来创建必要的文件。如果你不想手动创建文件结构，那么也可以直接复制存储库以获取所需的文件。

![image](https://github.com/user-attachments/assets/3560ef10-5350-42a1-bac5-4b009e23fe64)


[图6 - 5文件结构，展示Root Directory/下models/（# To store LLM bin files）、notebooks/（# Jupyter Notebooks for experimenting with LLMs ）、temp/（# for writing uploaded files for Loader ）、app.py、pipfile、run_app.bat、setup_env.bat ]


在models文件夹中存储下载的LLM，而pip文件将位于根目录中。

为了在虚拟环境中创建并安装所有依赖项，可以在根目录中使用pipenv install命令，或者直接运行setup_env.bat批处理文件。这将从pipfile中安装所有依赖项。这样做可以确保在虚拟环境中安装所有必要的包和库。一旦成功安装依赖项，我们就可以进行下一步——下载所需的模型。

#### 6.4.2 Llama 2和LangChain的交互流程

本节将利用LangChain开发一个应用程序，该应用程序将使用Llama 2，主要通过文本与模型进行交互。简单来说，我们可以将大多数模型视为“文本输入、文本输出”的形式。因此，LangChain的许多接口都是围绕文本展开的。

1. **加载数据集**：LangChain针对不同格式的数据源内置了各种解析脚本。最终这些数据都将转换为纯txt文本格式，以实现文本标准化。

```python
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

loader = UnstructuredFileLoader("./文档地址")
docs = loader.load()
```

2. **文本切分**：


在进行文本切分时，chunk_size用于指定切分后的文本块字数，而chunk_overlap则用于指定文本块间的重叠字数。由于“鸡汤”文本总长度较短且内部语义关联度高，因此将chunk_size设为50，将chunk_overlap设为20。


```python
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

loader = UnstructuredFileLoader("./文档地址")
docs = loader.load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=50,chunk_overlap=20)
docs = text_splitter.split_documents(docs)
```


3. **文本嵌入**：


在完成文本切分之后，我们需要对其进行向量化表示，即将其映射为低维稠密向量，并将这些向量存储到向量数据库中。此处选用的向量数据库是无须注册的FAISS。
```python
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
import os

embeddings = HuggingFaceEmbeddings(
model_name = "{你的地址}/text2vec-large-chinese",
model_kwargs = {'device': 'cuda'})

if os.path.exists("{你的地址}/my_faiss_store.faiss") == False:
vector_store = FAISS.from_documents(docs,embeddings)
vector_store.save_local("{你的地址}/my_faiss_store.faiss")
else:
vector_store = FAISS.load_local(
"{你的地址}/my_faiss_store.faiss",
embeddings=embeddings)
```
4. **加载模型**


   在加载模型时，通过LangChain来加载Llama 2并进行实例化。


```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# 加载tokenizer
tokenizer = AutoTokenizer.from_pretrained(
'/opt/Llama-2-7b-chat-hf',
trust_remote_code=True)

# 加载Llama 2
base_model = AutoModelForCausalLM.from_pretrained(
"/opt/Llama-2-7b-chat-hf",
torch_dtype=torch.float16,
device_map='auto',
trust_remote_code=True
)
llm = base_model.eval()
```


5. **语义检索**

   
- **向量化召回**：


FAISS默认采用L2（欧氏距离）作为度量，根据相似度对召回的文档进行降序排序。


```python
query = "根据文档内容，回答项目信息？"
# 计算相似度，结果中相似度较高的文本块排在前面
docs = vector_store.similarity_search(query)
context = [doc.page_content for doc in docs] # 提取文本块中的文本内容
print(context)
```
- **设置提示模板**：

以下是Llama 2默认的提示模板。

```python
qa_template = """Use the following pieces of information to answer the user's question.
#If you don't know the answer, just say that you don't know, don't try to make up an answer.
#Context: {context}
#Question: {question}
#Only return the helpful answer below and nothing else.
#Helpful answer: """
```

我们可以参考上面的提示模板，根据实际场景定制自己的提示模板，用于拼接查询内容和召回结果。

```python
context = "\n".join(context)
prompt = f"基于以上内容：\n{context} \n 请回答：{query} \n 字数限制在30字以内"
```

6. **推理示例**：


为Llama 2设置参数，包括最大令牌（max_new_tokens）、最高k值（top_k）、温度（temperature）及重复惩罚（repetition_penalty）等，并将提示输入模型。
```python
inputs = tokenizer([f"Human:{prompt}\nAssistant:"], return_tensors="pt")
input_ids = inputs["input_ids"].to('cuda')

# llm参数设置
param_config = {
"input_ids":input_ids,
"max_new_tokens":1024,
"do_sample":True,
"top_k":5,
"top_p":0.95,
"temperature":0.1,
"repetition_penalty":1.3
}
result = llm.generate(**param_config)
answer = tokenizer.decode(result[0], skip_special_tokens=True)
print(answer)
```

#### 6.4.3 具体案例

假如我们想要使用Llama 2对一个PDF文件进行问答，通常先通过Python代码读取PDF文件的内容，然后将读取到的内容和问题发送给Llama 2进行总结。

如果文本超过了API最大的token限制，则会报错。此时，我们可以先对要发送的内容进行分段处理，如通过tiktoken计算并分割，然后将各段发送给Llama 2进行问答，最后对各段的回答内容进行汇总概括。

如果使用的是LangChain，那么它会很好地帮助你处理这个过程，使编写代码变得非常简单。这些代码将包括以下内容。

（1）导入文档：create_vector_db。

（2）提示选择：set_custom_prompt。

（3）导入模型：load_llm。

（4）问题设置：qa_bot。

（5）问答结果：final_result。



这样你就可以轻松地完成对一个PDF文件进行问答的任务。示例代码如下：

```python
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain import PromptTemplate
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import CTransformers
from langchain.chains import RetrievalQA

DATA_PATH = 'data/'
DB_FAISS_PATH ='vectorstore/db_faiss'

#自定义提示
custom_prompt_template = """Use the following pieces of information to answer the user's question.
If you don't know the answer, just say that you don't know, don't try to make up an answer.

Context: {context}
Question: {question}

Only return the helpful answer below and nothing else.
Helpful answer:
"""

# 创建一个向量数据库函数
""" 该函数用于创建向量数据库。首先从指定路径加载PDF文件，其次使用文本分割工具将文档拆分成小块，再次使用HuggingFaceEmbeddings模型将文本块转换为向量，最后将向量存储在FAISS中并保存到本地 """
def create_vector_db():
    loader = DirectoryLoader(DATA_PATH,
                              glob='*.pdf',
                              loader_cls=PyPDFLoader)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,
                                                   chunk_overlap=50)
    texts = text_splitter.split_documents(documents)
    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',
                                        model_kwargs={'device': 'cpu'})
    db = FAISS.from_documents(texts, embeddings)
    db.save_local(DB_FAISS_PATH)

#设置自定义提示函数
""" 该函数用于设置自定义提示。创建一个PromptTemplate对象，包含预定义的输入变量和模板 """
def set_custom_prompt():
    """
    Prompt template for QA retrieval for each vectorstore
    """
    prompt = PromptTemplate(template=custom_prompt_template,
                            input_variables=['context', 'question'])
    return prompt

# 创建问题链函数
""" 该函数根据传入的参数创建一个问题链。使用RetrievalQA类创建一个问题链对象，包含llm模型、检索工具和其他相关参数 """
def retrieval_qa_chain(llm, prompt, db):
    qa_chain = RetrievalQA.from_chain_type(llm=llm,
                                           chain_type='stuff',
                                           retriever=db.as_retriever(search_kwargs={'k': 2}),
                                           return_source_documents=True,
                                           chain_type_kwargs={'prompt': prompt})
    return qa_chain

#加载Llama 2函数
""" 该函数用于加载Llama 2。创建一个CTransformers对象，包含模型名称、模型类型等参数 """
def load_llm():
    llm = CTransformers(
        model = "Llama-2-7b-chat-hf",
        model_type="llama",
        max_new_tokens = 512,
        temperature = 0.5
    )
    return llm

#创建一个问答机器人函数
""" 该函数用于创建一个问答机器人。首先加载预训练好的向量数据库，其次加载Llama，再次设置自定义提示并创建一个问题链，最后返回问题链对象 """
def qa_bot():
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2",
                                        model_kwargs={'device': 'cpu'})
    db = FAISS.load_local(DB_FAISS_PATH, embeddings)
    llm = load_llm()
    qa_prompt = set_custom_prompt()
    qa = retrieval_qa_chain(llm, qa_prompt, db)
    return qa

#获取基于PDF文件的问答结果函数
""" 该函数用于获取基于PDF文件的问答结果。首先创建一个问答机器人，其次将查询传递给问题链，最后返回问题链的响应结果 """
def final_result(query):
    qa_result = qa_bot()
    response = qa_result({'query': query})
    return response

#初始化问答流程，发送问题
""" 首先创建一个问答机器人，其次发送一条消息以启动机器人，最后更新消息内容并将问题链对象存储在用户会话中 """
async def start():
    chain = qa_bot()
    msg = cl.Message(content="Starting the bot...")
    await msg.send()
    msg.content = "Hi, Welcome to Medical Bot. What is your query?"
    await msg.update()
    cl.user_session.set("chain", chain)

#主函数，处理消息并返回答案
""" 这是程序的异步主函数。首先从用户会话中获取问题链对象，其次使用异步回调处理器处理消息，最后获取答案和相关来源并将其发送给用户 """
async def main(message):
    chain = cl.user_session.get("chain")
    cb = cl.AsyncLangchainCallbackHandler(
        stream_final_answer=True, answer_prefix_tokens=["FINAL", "ANSWER"]
    )
    cb.answer_reached = True
    res = await chain.acall(message, callbacks=[cb])
    answer = res["result"]
    sources = res["source_documents"]
    if sources:
        answer += f"\nSources:" + str(sources)
    else:
        answer += "\nNo sources found"
    await cl.Message(content=answer).send()
```
