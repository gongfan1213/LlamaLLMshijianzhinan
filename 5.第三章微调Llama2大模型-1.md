### 第3章 微调Llama 2大模型

在经过预训练后，大模型可以获得完成各种任务的通用能力。然而，越来越多的研究表明，大模型的能力可以通过进行针对特定目标的微调进一步提高。微调可以使大模型适应特定任务，从而提高其性能和效率。通过微调，大模型可以更好地理解特定领域的语言和术语，并且更准确地预测结果。此外，微调还可以帮助大模型更好地适应不同的数据集和任务，从而提高其泛化能力和鲁棒性。因此，微调技术已经成为大模型应用中不可或缺的一部分。

微调技术是一种对大模型进行调整，以使其适应特定任务的技术。目前，微调技术主要有以下两种。

- **指令微调**：目标是提高大模型的能力。

- **对齐微调**：目标是将大模型的行为与人类的价值观或偏好对齐。

OpenAI发布的ChatGPT就应用了微调技术，从而获得了非常好的效果。本章主要针对指令微调进行详细的介绍。

指令微调是一种在自然语言格式的实例集合上微调预训练后的大模型的技术。这种技术与有监督微调和多任务Prompt训练密切相关。为了进行指令微调，首先需要收集或构造指令格式的数据。这些数据有各种来源，如人工标注的数据集、自动生成的数据集或从现有数据集中提取的样本。其次需要使用这些格式化的实例以有监督的方式微调大模型，如使用序列到序列的损失对大模型进行训练。

目前开源的大模型，如Llama 2，经过指令微调后，展现出了极高的泛化能力，即使在多语言场景下也有不错的表现。这种泛化能力使大模型可以适应各种任务和数据集，从而提高其性能和效率。此外，指令微调还可以帮助大模型更好地理解特定领域的语言和术语，从而提高其预测准确性和可靠性。因此，指令微调已经成为大模型应用中不可或缺的一部分。

### 3.1 微调的数据集准备和标注

在如今的AI领域，开源的大模型，如Llama 2，已经在各种NLP任务中取得了显著的成果。为了充分发挥这些大模型的潜力，往往需要对它们进行微调，以使其适应特定的应用场景。在微调过程中，数据集的准备和标注是至关重要的环节。本节以Llama 2大模型为例，详细介绍微调的数据准备和标注的步骤。

### 3.1.1 数据集准备

通常情况下，一个指令格式的实例数据包括一个任务描述（被称为指令（instruction））、一个输入和一个输出。指令描述大模型需要完成的任务，输入是大模型需要处理的原始数据，输出是大模型对输入数据的预测结果。这种指令格式的实例数据可以帮助大模型更好地理解特定任务的需求和目标，从而提高其性能和效率。

数据集一般可通过以下三种途径构建。

- **格式化已有数据集**：将传统的NLP数据集按照特定的格式进行调整，用于指令微调。为降低格式化成本，可以通过ChatGPT生成指令。一般Prompt：请你为这段内容生成一个合理的问题。用户可以根据这个Prompt与ChatGPT进行交互，以获得符合要求的数据。

- **人工标注数据集**：采用这种方法构建的数据集更加符合人类的语言习惯，可以获得更好地与人类的价值观或偏好对齐的效果。为降低人工标注成本，目前很多数据集由ChatGPT生成，包括用户分享的ChatGPT对话历史（如ShareGPT数据集）和使用ChatGPT蒸馏生成的数据集。 

- **复用开源数据集**：复用开源数据集，从而增加微调任务的多样性，提高大模型的性能。

数据集可分为通用数据集和专用数据集。前者目前有大量开源数据集可供使用。通过使用通用数据集，我们可以训练大模型以使其具备通用任务场景的处理能力。后者是针对某个领域的数据集，可以根据具体需求和任务场景自行构建（如法律数据集、医疗数据集等），以满足特定领域任务场景的理解和处理需求。通过使用专用数据集，我们可以提供更加精准和定制化的指令，从而提高大模型在特定领域的性能。

除了使用常规的数据集，引入多样性的数据集也可以有效提高大模型的性能，如CoT等。这些数据集可以帮助大模型更好地理解和处理复杂的语言结构和推理任务，从而提高大模型的性能和泛化能力。

因此，通过使用通用数据集、专用数据集，以及引入多样性的数据集，可以有效提高大模型的性能。这种综合使用不同类型数据集的方法已经得到了广泛的应用，并取得了显著的成果。

一般来说，我们采用上述方法得到数据集后，还需要进行以下处理，才能得到高质量的数据。

- **数据清洗**：目的是删除数据中的低质量部分，包括文本清理（去除换行符、字母大小写转换、URL删除、HTML标签删除等），可参考一些开源工具的自动化处理；基于元信息过滤，如OpenAI筛选Reddit链接时过滤掉点赞数小于3的帖子。还可使用一些数据质量检查方法自动删除低质量数据。

- **数据过滤**：目的是过滤掉不符合模型训练目标的文本，可采用不同的过滤器实现。典型的包括长度过滤（考虑大模型上下文限制和训练目标，数据不能过短或过长）、语言过滤（视目标模型多语言支持目标，过滤特定语言内容）、机器生成文本过滤（过滤Google翻译文本、ChatGPT生成文本等 ）。 

- **数据去重**：训练数据中的重复数据会极大地降低模型性能，针对微调训练集，可使用向量检索方式进行数据去重。 

- **价值观控制**：从数据中过滤掉不符合所在国家法律或道德要求的内容，如涉及种族歧视、语言暴力等内容，以所在国家法律法规为准。 

- **个人信息脱敏**：从数据中剔除受法律保护的个人信息数据，如ID、医疗记录等。 

### 3.1.2 数据集标注

通过上述方法，我们得到了一批高质量的数据，如果要将这些数据用于大模型的微调，还需要将其转换成大模型特定的输入格式（这一步根据不同大模型的情况来处理）。以Llama 2大模型为例，一般将数据处理成json格式或csv格式的文件，典型的数据格式如下：

```json

{"instruction": "", "input": "", "output": ""}

```

Llama 2大模型的指令微调格式示例如下：

```json
{
    "instruction": "提供器材，建议一套锻炼计划。",
    "input": "引体向上杠，运动垫",
    "output": "运动计划：\n\n1. 热身：开始进行5分钟的动态拉伸，如手臂环绕、腰摆动或跳跃。\n\n2. 上半身循环：在引体向上杠和运动垫上做以下几个动作，每个动作做3轮，每轮10次；引体向上、俯卧撑、三头肌二头肌撑、平板支撑和侧板支撑。\n\n3. 下半身循环：在运动垫上做以下几个动作，每个动作做3轮，每轮10次；桥式、深蹲、侧腿弓步和后蹲步。\n\n4. 核心循环：在运动垫上做以下几个动作，每个动作做3轮，每轮10次；仰卧起坐、腿部抬高、斜方肌扭转和背部伸展。\n\n5. 冷却：最后进行5分钟的静态伸展运动，如眼镜蛇式、猫/牛式或前屈。"
}
```
```json
{
    "instruction": "列举两种人类减小对环境的影响的方法",
    "input": "",
    "output": "人们可以通过减少能源消耗和减少废物产生来减小对环境的影响。通过使用节能家电、节能灯泡和节油车辆，以及回收、堆肥和再利用等方式，人们可以做出更可持续的选择，对环境产生积极的影响。"
}
```

其中，“instruction”（指令）通常是指对大模型的任务描述或指导。它可以是一个问题、一个命令、一个任务描述等，用于告诉大模型需要完成什么样的任务或目标。

“input”（输入）是指输入大模型的数据。对于NLP任务，输入可以是一个文本、一句话、一个图像等。大模型会根据输入的数据进行处理和分析。

“output”（输出）是指大模型根据输入数据生成的结果。对于NLP任务，输出可以是一个回答、一个分类标签、一个生成的文本等，取决于具体的任务和模型设计。

此外，还有一种对话微调（Conversation Tuning）格式。对话微调是一种特殊的指令微调，其目的是让大模型在“补全”能力的基础上，解锁“对话”能力。

典型的对话微调格式如下：

```json
<s>Human: "+问题+"\n</s><s>Assistant: "+答案
```
示例：
```json
<s>Human: 用一句话描述地球为什么是独一无二的。</s><s>Assistant: 因为地球是目前为止唯一已知存在生命的行星。</s>
```
其中，“<s>Human: ”+问题表示用户输入的问题，“\n</s><s>Assistant: ”+答案表示对话系统返回的答案。这段代码的意思是，将用户输入的问题和对话系统返回的答案以特定的格式输出。

### 3.2 Llama 2大模型加载

Llama 2是一个基于PyTorch的预训练大模型，可以用于各种NLP任务，如文本分类、命名实体识别、情感分析和对话聊天等。本节将介绍如何加载Llama 2大模型。

1. **安装PyTorch库和Transformers库**

在使用Llama 2大模型之前，需要安装PyTorch库和Transformers库，可以在命令行中使用以下命令对其进行安装：

```bash
pip install torch
pip install transformers
```

2. **下载Llama 2大模型**

可以从Llama 2的GitHub页面中下载预训练大模型。在本示例中，我们将使用Llama2-base模型。

    - **Llama2-base模型**：Llama2-base模型有7B、13B和70B三种大小，如下表所示。
### 表3-1 Llama2-base模型
|模型名称|模型加载名称|
| ---- | ---- |
|Llama2-7B|meta-llama/Llama-2-7b-hf|
|Llama2-13B|meta-llama/Llama-2-13b-hf|
|Llama2-70B|meta-llama/Llama-2-70b-hf|
    - **Llama2-Chat模型**：Llama2-Chat模型基于Llama2-base模型进行了监督微调，具有更强的对话能力。Llama2-Chat模型也有7B、13B和70B三种大小，如下表所示。
### 表3-2 Llama2-Chat模型

|模型名称|模型加载名称|
| ---- | ---- |
|Llama2-7B-Chat|meta-llama/Llama-2-7b-chat-hf|
|Llama2-13B-Chat|meta-llama/Llama-2-13b-chat-hf|
|Llama2-70B-Chat|meta-llama/Llama-2-70b-chat-hf|

以Llama2-7B模型为例，可以使用以下命令对其进行下载：

```bash
git lfs install
git clone https://huggingface.co/meta-llama/Llama2-7b-hf
```

3. **加载Llama 2大模型**

在Python中加载Llama 2大模型，需要使用Transformers库中的AutoModelForCausalLM类和AutoTokenizer类加载Llama 2大模型及词表相关配置文件。可以使用以下命令加载Llama 2大模型：

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
model_path = "meta-llama/Llama2-7b-chat-hf"
model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto',
                                             torch_dtype=torch.float16, load_in_8bit=True)
tokenizer = AutoTokenizer.from_pretrained(model_path,
                                          use_fast=False)
```

在这里，我们先指定了大模型的名称和路径，然后使用AutoTokenizer.from_pretrained()方法加载预训练的tokenizer，并使用AutoModelForCausalLM.from_pretrained()方法加载预训练大模型。

至此，Llama 2大模型加载完成。


### 3.3 微调策略设计及模型重新训练

微调策略设计及模型重新训练是深度学习领域中的重要内容，对于提高模型性能、使模型适应新任务或数据分布变化至关重要。本节以Llama 2大模型为例，探讨如何在上下文中进行微调策略设计及模型重新训练。

### 3.3.1 微调策略设计

微调是指在预训练模型的基础上，使用少量的标注数据对模型进行进一步训练，以使其适应特定任务。在微调过程中，需要设计合适的微调策略，以达到更好的微调效果。

1. **全参数微调**

全参数微调的优点是可以使模型更好地适应特定领域或任务，提高模型的泛化能力。由于微调了所有的参数，模型可以更好地适应新的数据分布，因此提高了模型的性能。此外，全参数微调还可以避免欠拟合问题，因为在微调过程中，所有的参数都会被更新，而不是只更新部分参数。

然而，全参数微调需要大量的计算资源和时间，因为需要更新所有的参数，这对于计算资源有限的研究人员来说是一个很大的问题。此外，全参数微调还可能导致模型过拟合，因为在微调过程中，模型可能会过度拟合数据集，从而降低模型的泛化能力。因此，在实际应用中，需要根据具体情况选择微调方法，权衡微调的效果和计算资源和时间成本。

2. **参数高效微调**

随着模型参数量的不断增加，进行全参数微调的成本不断升高。这种高成本主要表现在对硬件资源的要求高、显存占用量大、训练速度慢、耗时长及存储成本高等方面。为了解决这些问题，参数高效微调（Parameter-Efficient Fine-Tuning，PEFT）方法应运而生。与全参数微调不同，参数高效微调只训练模型的一小部分参数，而不是所有的参数。参数高效微调有以下几个优点：对硬件资源的要求低，因为只训练模型的一小部分参数，所以显存占用量小；训练速度快，耗时短；存储成本低，不同任务可以共享大部分权重参数；可能会有更高的模型性能，可以避免过拟合问题。

下面对几种比较常见的参数高效微调方法进行介绍。

- **Prefix-Tuning**：是一种用于自然语言生成任务的参数高效微调方法，它可以通过微调预训练模型来生成具有特定前缀的文本。在Prefix-Tuning中，模型的输入由两部分组成：前缀和待生成的文本。前缀是指输入文本的前几个词或字符，而待生成的文本则是指在前缀的基础上，模型需要生成的文本。
- 
