### 第7章 多语言大模型技术介绍及其工业应用

随着Llama 2大模型的出现，围绕多语言大模型的研究在国内外社区火热展开。本章将从多语言大模型的研究现状和技术方向出发，系统地总结多语言大模型的训练资源和预处理、优化方向，以及多语言大模型现阶段典型的工业应用。



#### 7.2 多语言大模型的预训练资源和评测任务

数据资源在如今这个大模型时代与石油和煤炭在工业时代一样重要。本节将总结多语言大模型训练所使用的主流语料，以及如何对这些语料进行加工、提炼，以便更好地帮助研究人员提升多语言大模型的效果。



##### 7.2.1 多语言大模型的预训练资源介绍

1. **预训练资源**

大模型需要海量的训练语料才可以学习到更全面的知识和内容，所以现在越来越多的开源训练语料被用来训练大模型。本节按照训练语料的内容类型，简要地将目前被广泛使用的语料资源归类为书籍资源（Books）、网页资源（CommonCrawl）、Reddit资源（Reddit Links）、维基百科（Wikipedia）、代码（Codes）和其他（Other）[31]，如表7-2所示。

|语料资源|数据大小|数据源头|更新时间|
| ---- | ---- | ---- | ---- |
|BookCorpus|5GB|Books|2015-12|
|Project Gutenberg| - |Books|2021-12|
|C4|800GB|CommonCrawl| - |
|CC-Stories-R|31GB|CommonCrawl|2019-04|
|CC-NEWS|78GB|CommonCrawl|2019-09|
|REALNEWs|120GB|CommonCrawl|2019-02|
|OpenWebText|38GB|Reddit Links|2023-03|
|PushShift.io|2TB|Reddit Links|2023-03|
|Wikipedia|21GB|Wikipedia|2023-03|
|BigQuery| - |Codes|2023-03|
|The Pile|800GB|Other|2020-12|
|ROOTS|1.6TB|Other|2022-06|


（1）**书籍资源**：

BookCorpus[32]是以前的预训练模型（如GPT/GPT-2）经常使用的一个数据集，包括超过1.1万本书籍，覆盖广泛的类别（如小说和传记）。现阶段规模较大的一个书籍资源是Project Gutenberg[33]，包含超过7万本书籍，也被用于Llama及Llama 2模型的训练。

（2）**网页资源**：

CommonCrawl作为数据量（PB级）最大的开源网络爬取数据库之一，被广泛应用于大模型的训练。因为其语料资源丰富，所以现阶段的大模型只使用其中某些时间段的子集数据来进行训练。由于网络爬取数据中包含大量的噪声和低质量数据，所以在使用它前都会进行数据的清洗和预处理，现阶段有几个数据清洗后的多语言数据集可供选择：C4[34]、OSCAR[35]、CCAligned[36]。其中，C4的名字源于英文的Colossal Clean Craled Corpus，其英语版本被Llama模型使用，其多语言版本又称为mC4，被用来训练mT5模型。OSCAR是从CommonCrawl数据集中利用语言分类技术抽取出来的，用于ELMO模型的训练。CCAligned通过URL中不同国家的缩写生成了3.92亿个文档对，覆盖138种不同的语言。

（3）**Reddit资源**：

Reddit是一个类似于网络论坛的社交网络平台，用户可以在该平台上发表和回答自己想讨论的话题，并且给回答点“踩”或点“赞”，那些高赞资源的价值很高。OpenWebText就是通过爬取这些数据形成的数据集。另外一个数据集是PushShift.io[37]，这是一个实时更新的数据集，支持用户在整个数据集中进行检索、总结等操作，方便用户使用和处理其中的数据。

（4）**维基百科**：

维基百科[38]具有不同领域的海量高质量文章，多数文章都是百科类的，并且涵盖多种语言。Llama、GPT-3使用过英语版本的维基百科，mBERT和XLM-100使用过多语言版本的维基百科。但是因为多语言版本的维基百科中的文档数量比CommonCrawl数据集少很多，因此并没有被多语言大模型广泛使用。WikiMatrix[39]是一个从维基百科数据中抽取的平行语料，共有1.35亿条平行语料，覆盖1620个语对和85种语言，主要用于机器翻译。

（5）**代码**：

对于代码数据集，现有的工作主要集中在从网络上爬取具有开源协议的代码。代码主要有两个爬取源：第一个是公开的代码库，如GitHub；第二个是与代码相关的问答平台，如Stackflow、CSDN。Google已经开源了BigQuery[40]，其包含一定数量、不同编程语言的开源代码。作为代码大模型，CodeGen[41]就使用了BigQuery进行训练，而BigQuery的多语言版本也被用来训练CodeGen-Multi。

（6）**其他**：

其他语料资源中最有特点的是The Pile数据集和ROOTS数据集。The Pile数据集包括从上述多个数据源中抽取的800GB的数据集，涵盖22个高质量数据源的子集合。The Pile数据集被各种规模的大模型使用，如GPT-J（6B）和OpenLLaMA（7B/13B） 。ROOTS数据集是一个多语言数据集，由很多更小的数据集组成，涵盖59种语言。多语言大模型BLOOM就采用了ROOTS数据集进行训练。此外，还有一个数据集Paracrawl[42]利用Bitextor工具从网页中挖掘与英语平行的句对，其主要覆盖欧洲的语言且包括9种低资源语言。


2. **预训练资源质量**

保证数据质量是训练出性能出色的多语言大模型的关键，鲁棒性高的多语言大模型能在跨语言的下游任务中表现得更好。上文介绍的多语言预训练数据集大多来源于网络的自动挖掘，其数据质量无法得到很好的保证，因此从不同维度对这些从网络中挖掘的语料进行评估很有必要[43]。

在ACL 2023关于多语言大模型的Tutorial中[44]，微软的研究人员提出对多语言预训练数据集从多语言分布、数据质量、来源和监管4个方面进行质量评价。在多语言分布方面，研究人员以CommonCrawl数据集为例，指出虽然其包含100多种语言，但是其中57种语言的数量小于0.001%，所以在利用其构造多语言数据集时要考虑是否自己生成与下游任务语言一致的数据。

在数据质量方面，建议构造数据集的研究人员不要只关注那些低资源语料，也要注意那些语言被错误分类的样本，它们有可能是因为与其他语言相似及质量太差而被划分到一些高资源语言类别中的。同时要注意那些机器生成的语料和因为识别工具效果有限而包含色情或不良信息的语料。研究人员建议从数据的数量、质量、领域数量、可持续性、共享性等维度评估多语言预训练数据集的质量。

##### 7.2.2 评测任务介绍

7.2.1节介绍的数据集主要集中在无监督数据集上，本节将介绍在多语言有监督数据集上评测多语言大模型的Benchmark。Benchmark中的任务主要包括文本分类任务（单句或句对）、QA任务、序列标注任务和文本生成任务，如图7-4所示。

![image](https://github.com/user-attachments/assets/593a17d2-34f0-46a7-a415-70ed41f85b1e)


|任务类型|具体任务及相关数据集|
| ---- | ---- |
|文本分类（Sentence Pair Classification）|XNLI、Indic-XNLI、Americas-XNLI、PAWS-X、XCOPA、IndicCOPA、XStoryCloze|
|问答（Question Answering）|XQuAD、MLQA、TyDiQA-GoldP、IndicQA、TyDiQA-Primary、XDOR-QA、LAIQA|
|序列标注（Sequence Labelling）|UDPOS、MasakhanePOS、PAN-X、MasakhaneNER、IndicNER|
|文本生成（Generation）|XLSum、IndicParaphrase、QuestionGeneration、TyDIQA-QG|

1. **评测数据集**

构造鲁棒性高并且全面的评测任务可以帮助我们更好地理解大模型的效果，这类评测在英语中是十分活跃的研究领域，如GLUE及难度上升的SuperGLUE评测数据集Benchmark，以及近期在其基础上建立的多语言评测数据集XTREME、XTREME-R和XGLUE等。多语言评测数据集的目标是覆盖更多样的任务和语言，这样我们就可以更好地评估多语言大模型的泛化性能。与此同时，也有一些研究人员致力于构建针对特定语言的评测数据集，如针对印度语的IndicXTREME等。

多语言评测数据集的构造基本上都缺乏语言的多样性，并且很少覆盖低资源语言目标。在大模型出现以前，通常将英语评测数据集通过机器翻译的方式翻译成多种目标语言评测数据集。这种方法无法生成自然的、有代表性的目标语言，并且会影响评估的有效性。为了缓解这些问题，更好地衡量多语言大模型在跨语言的零样本和小样本任务中的性能，多家研究机构合作构建了xP3[18]评测数据集，覆盖46种语言，包含英语及机器翻译的提示；微软的团队构建了MEGA[45]评测数据集，包含16个评测任务，覆盖70多种语言；华盛顿大学、Google及Allen AI联合发布了多语言评测数据集BUFFET[46]，其包含15个评测任务，覆盖54种语言，并且额外提供固定的小样本集合及指令，可用于更好地衡量多语言大模型在小样本跨语言迁移任务中的效果。

除此之外，在ACL 2023上，也有研究人员发布了面向任务型对话的多语言对话数据集GlobalWoZ[47]和X-RiSAWOZ[48]，以帮助开发人员更好地开发和评价多语言任务型对话系统。

2. **评测方法**
传统模型的评测方法（Evaluation Methodologies）利用有监督训练语料在预训练模型上进行微调，链路为预训练+微调。多语言大模型具备天生的零样本和小样本学习能力，其评测方法可以分成两大类：一类是使用原有的评测方法进行任务相关的微调；另一类是基于提示的ICL，链路变为预训练+提示+预测。任务相关的微调需要利用训练语料对模型参数进行更新，而ICL不需要更新模型参数，只需设计不同的提示模型就会返回对应的结果。多语言大模型的评测方法如图7-5所示。



|评测方法分类|具体方法|
| ---- | ---- |
|任务相关的微调（Task-Specific Finetuning）|Zero-Shot Cross Lingual Transfer、Few-Shot Cross Lingual Transfer、Monolingual Finetuning、Translate-Train|
|基于提示的ICL（Prompting/In-Context Learning）|Monolingual Prompting、Cross Lingual Prompting、Translate Test Prompting、Chain-of-Thought Prompting、Cross-Thought Prompting|

![image](https://github.com/user-attachments/assets/38230def-f33c-4e43-9684-d793cf7160bd)


（1）**任务相关的微调（Task-Specific Finetuning）**
    - **Zero-Shot Cross Lingual Transfer**：先在一种语言上进行
    任务相关的微调，然后利用另外一种语言的测试集进行评测。
    - **Few-Shot Cross Lingual Transfer**：先在英语和少量目标语言上进行参数微调，然后利用目标语言的测试集进行评测。 
    - **Monolingual Finetuning**：只在全量的目标语言上进行参数微调。 
    - **Translate-Train**：使用经过机器翻译的目标语言进行参数微调。

（2）**基于提示的ICL（Prompting/In-Context Learning）**

![image](https://github.com/user-attachments/assets/06670995-9d85-4c17-95b5-cd78bf4b2f5c)


这种评测方法希望构造适当的提示去激活大模型能力，帮助人们解决当前的任务，主要从4个维度（提示经模板泛化的任务相关小样本（Task Example）、输入（Input）、测试数据（Template），以及答案（Answer） ）输入信息。图7-6所示为不同任务的提示和小样本数据模板。
    - **Monolingual Prompting**：提示、小样本输入、测试数据和答案都是目标语言。 
    - **Cross Lingual Prompting**：提示、小样本输入是源语言，测试数据和答案是目标语言。 
    - **Translate Test Prompting**：提示、小样本输入是源语言，测试数据是由源语言翻译而成的目标语言，答案是目标语言。 
    - **Chain-of-Thought Prompting**：对于一些注重多步逻辑推理的问题，在提示中加入Think Step-by-Step有助于提高大模型回答的准确率，这类提示技术中的提示、小样本输入、测试数据和回答都是目标语言。 
    - **Cross-Thought Prompting**：这类技术利用大模型擅长处理英语的特点，输入使用目标语言，先在提示中让多语言大模型将问题复述成英语，然后利用英语进行CoT推理，最终的回复也是英语。实验证明，这类提示技术在跨语言推理上的效果比输入只用英语或将目标语言翻译成英语都要好。

#### 7.3 多语言大模型的优化方向

介绍完多语言大模型的预训练资源和评测任务，本节将介绍数据预处理流程和Tokenizer，以及各类多语言大模型的结构和效果。

##### 7.3.1 数据预处理流程

多语言大模型的预训练数据清洗和单语言大模型并没有显著的不同，如图7-7所示，都要经历数据的收集、初次数据清洗（语言检测）、文档去重及质量过滤（隐私过滤）等步骤，唯一的区别是，在初次数据清洗过程中，需要设置合理的阈值，识别出我们需要的语言并保留一定比例的多语言文档用于模型训练。

|流程步骤|具体内容|
| ---- | ---- |
|Collection|Downloading、Text Extraction、Site Development (URL Based)|
|Initial Cleaning|Language Identification、Threshold based filtering、Multi-language documents|
|Deduplication|Recent Substring based (nc4, OSCAR*, CC100)、Fuzzy Minhash based (GPT-3, ThePile)、Both (Refined Web)|
|Filtering|Heuristics Based (Refined Web)、Model Based (CC-Net, CC100)、NSPW URL Based, Pi、Line Based, Doc based|

![image](https://github.com/user-attachments/assets/e73990a7-bfb1-42dc-91a2-2365b7020b25)



##### 7.3.2 Tokenizer

经历过数据预处理的语料，需要经过Tokenizer的处理后才可以变成多语言大模型直接可用的数据格式，下面我们介绍一下常用的Tokenizer及其在多语言上的潜在问题。


1. **Tokenizer选择**

主流的Tokenizer一共分为两种：一种是基于子词的Tokenizer，其中有代表性的有以下两种。

（1）**BPE算法[49]**：BPE算法会学习如何将两个最常见、连续并且在训练字典中的token合并，将其作为新的token加入字典，直到字典容量满足要求为止。这个算法也被GPT、BLOOM和Llama系列模型使用。

（2）**Sentencepiece[50]**：Sentencepiece是一个开源的代码库，实现了如BPE、UnigramTokenization等Tokenization算法，同时也实现了非子词的Tokenizer，如基于字和字母的Tokenizer。XGLM、mT5、PaLM系列模型都使用Sentencepiece作为Tokenizer。



另一种是基于字节的Tokenizer（BBPE）。当我们的字典容量有限时，以字节为最小token进行分词，这种分词方法可以编码所有可能的序列信息[51]，并且对拼写有很好的容错性。有研究[52]使用UTF-8的编码训练了一个没有子词的多语言大模型mT5，在多语言测试集上展现出了较好的效果。这种分词方法可以用来训练没有子词的大模型，如拉丁语系语言的模型，因为UTF-8编码拉丁语系语言的长度会比编码汉语、日语和韩语要小[53]。但是这种分词方法需要更深的编码器，并且要求大模型可以接收更长的上下文信息。Llama 2大模型对于非UTF-8的字符就使用这种分词方法。


2. **Tokenizer在多语言上的潜在问题**

与单语言大模型相比，多语言大模型更加注重Tokenizer的质量，因为多语言大模型要在词表有限的情况下利用token表示更多的语言序列。有研究[7]从子词的分裂度（Fertility）计算一个自然词被分裂成子词的平均长度，最小的分裂度是1，表示每个自然词就是一个子词。从图7-9中可以看到，多语言大模型mBERT的分裂度在英语（EN）、汉语（ZH）、印尼语（ID）和日语（JA）上与单语言大模型基本相同，但是在韩语（KO）、阿根廷语（AR）、芬兰语（FI）、俄罗斯语（RU）和土耳其语（TR）上明显高于单语言大模型。这是因为mBERT的训练语料中包含大量的英语等高资源语言，而英语与阿根廷语、芬兰语、俄罗斯语和土耳其语等相比，是一种弱词形变化语言，当Tokenizer在多种语言间使用同一个词表时，会导致Tokenizer不能在对应的低资源语言中学到最优的分词组合。




随着针对某些语言子词的分裂度增大，我们不得不考虑多语言大模型的推理成本。有研究表明[54]，在低资源语言上，利用基于提示的ICL调用多语言大模型进行预测，会大大增加使用成本，因为我们会
