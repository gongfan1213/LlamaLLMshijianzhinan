### 第6章 Llama 2 + LangChain文档问答模型构建

本章将从理论入手，从零开始利用Llama 2和LangChain构建一个面向文档的问答模型。整个过程可分为以下几个步骤。

（1）理解Llama 2和LangChain的基本原理与功能。

（2）介绍LangChain及其如何与Llama 2协同工作。

（3）配置LangChain在各个平台（如Windows、macOS、Linux）上的运行环境。

（4）使用Llama 2创建模型，随后加载这个模型。

（5）借助LangChain处理输入的问题和文档内容。我们可以通过设定提示来设置并调整自定义提示。 

（6）运行问答模型，对于每个问题，它将为我们提供详细的答案。



通过以上步骤，我们可以逐步构建并优化问答模型，使其能够更有效地回答与文档内容相关的问题。

### 6.1 LangChain介绍

大模型（如GPT系列模型）在AI领域引发了技术革命。应用开发者利用这些大模型进行各种尝试，虽然已经开发出了许多有趣的应用，但单独使用这些大模型往往难以开发出功能强大的实用应用。

LangChain通过将大模型与其他知识库、计算逻辑相结合，实现了更强大的AI应用。可以将LangChain视为开源版的GPT插件，它提供了丰富的大模型工具，可以在开源大模型的基础上快速增强大模型的能力。

我们对LangChain进行了研究，并总结了一些重要内容。LangChain的出现使语言技术的应用更加活跃多元，LangChain有望在AI领域发挥重要作用，推动人们工作效率的变革。我们正处在AI应用爆发的前夜，积极拥抱新技术将为人们带来全新的体验。

### 6.2 LangChain的主要概念与示例

LangChain提供了一系列工具，可以帮助我们更好地使用大模型。总体而言，这些工具可以分为以下6种类型。

- **模型（Models）**：LangChain支持的各种模型类型和模型集成。

- **提示（Prompts）**：包括提示管理、提示优化和提示序列化。 

- **索引（Indexes）**：当索引与自己的文本数据结合使用时，大模型往往更加强大。 

- **内存（Memory）**：内存是在链和代理调用之间保持状态的概念。LangChain提供了一个标准的内存接口、一组内存实现及使用内存的链/代理示例。 

- **链（Chains）**：链不仅包括单个大模型调用，还包括一系列调用（如调用大模型和调用不同的实用工具）。LangChain提供了一个标准的链接口、许多与其他工具的集成，还提供了用于常见应用程序的端到端的链调用。 

- **代理（Agents）**：代理涉及大模型做出行动决策、执行该行动、查看一个观察结果，并重复该过程直到用户提出的任务完成为止。LangChain提供了一个标准的代理接口、一系列可供选择的代理及端到端代理的示例。 



LangChain模块图如图6 - 1所示。

![image](https://github.com/user-attachments/assets/c8afec55-d196-4938-aaf5-c6a5ad1ffe66)


[图6 - 1 LangChain模块图，展示Components下Models（LLMs、Chat Models、Text Embedding Models ）、Prompts（Prompt Templates ）、Indexes（Document Loaders、Text Splitters、Vector Stores、Retrievers ）、Memory（Chat Message History ）、Chains（Chain、LLM Chain、Indes - Related Chains ）、Agents（Tools、Agents、Toolkits、Agent Executor ）]

#### 6.2.1 模型

LangChain的核心价值在于提供了标准的模型接口。我们可以自由地切换不同的模型，目前主要有两种类型的模型可供选择。然而，考虑到一般用户的使用场景，我们以文本生成模型作为默认选择。提到模型，大家通常会想到ChatGPT。简单来说，模型的作用就是生成文本内容。

1. **语言模型**：实现文本生成的语言模型主要有两种。

- **通用语言模型**：接收一个文本字符串作为输入，并返回一个文本字符串作为输出。 

- **聊天模型**：接收一条聊天消息作为输入，并返回一条聊天消息作为输出。 



以下是使用Python编写的代码示例：

```python
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI

ChatOpenAIIlm = OpenAI()
chat_model = ChatOpenAI()
print(llm("say hi!"))
print(chat_model.predict("say hi!"))
```

2. **文本嵌入模型**：文本嵌入模型（Text Embedding Model）可以把文本转换为浮点数形式的描述。文本嵌入模型接收文本作为输入，并返回一组浮点数作为输出。这些浮点数通常用于表示文本的语义信息，可用于完成文本相似性计算、聚类分析等任务。通过使用文本嵌入模型，应用开发者可以构建更丰富的文本关联性，并提高基于大模型的应用性能。



以下是使用Python编写的代码示例：

```python
from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()
text = "This is a test document."
query_result = embeddings.embed_query(text)
doc_result = embeddings.embed_documents([text])
print(doc_result)
```
#### 6.2.2 提示

提示是指人们与模型进行交互的方式，也可以指模型的输入。使用提示可以引导模型生成符合人们期望的内容，如按照特定的格式返回数据。提示在人们与模型的交互过程中起到关键的作用。
LangChain提供了一系列工具，使构建人们想要的提示变得更加容易。以下是主要工具的介绍。

1. **提示模板**：LangChain的提示模板（PromptTemplate）是一种可以重复使用的提示模板，它可以帮助人们生成多个相关的提示。每个提示模板都包含一个文本字符串，该字符串定义了一组参数，通过这些参数可以生成提示。提示模板包含以下内容。

- 对语言模型的说明：提示模板会明确指定语言模型应该扮演的角色，以便更好地引导生成过程。 

- 一组示例：提示模板中会提供一组示例，这些示例可以帮助语言模型更好地理解人们的期望，从而生成更准确、更合适的内容。 

- 具体的问题：提示模板中会包含具体的问题，这些问题可以直接用于与语言模型进行交互，引导语言模型生成人们想要的内容。 



以下是使用Python编写的代码示例：

```python
from langchain import PromptTemplate

template = """
I want you to act as a naming consultant for new companies.
What is a good name for a company that makes {product}?"""

prompt = PromptTemplate(input_variables=["product"], template=template,)
prompt.format(product="colorful socks")
# -> I want you to act as a naming consultant for new companies.
# -> What is a good name for a company that makes colorful socks?
```

2. **聊天提示模板**：聊天提示模板（ChatPromptTemplate）是接收聊天消息作为输入的提示模板。聊天消息通常由不同的提示组成，并且每个提示都会有一个角色。



以下是使用Python编写的代码示例：

```python
from langchain.prompts import (
ChatPromptTemplate,
PromptTemplate,
SystemMessagePromptTemplate,
AIMessagePromptTemplate,
HumanMessagePromptTemplate,)
from langchain.schema import (
AIMessage,
HumanMessage,
SystemMessage)

template="You are a helpful assistant that translates {input_language} to {output_language}."
system_message_prompt = SystemMessagePromptTemplate.from_template(template)
human_template="{text}"
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)
chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])
print(chat_prompt.format_prompt(input_language="English", output_language="French", text="I love programming.").to_messages())
```

3. **案例选择器**：在处理多个案例时，可以使用案例选择器（ExampleSelector）来选择一个案例供提示使用。以下是一些常见的案例选择器。

- **自定义的案例选择器**：根据特定的规则或需求，手动选择一个案例作为提示的基础。 

- **基于长度的案例选择器**：根据输入的长度选择案例。当输入较短时，选择较少的案例；当输入较长时，选择较多的案例。 

- **相关性案例选择器**：根据输入与案例的相关性选择最相关的案例。通过比较输入与每个案例的相似度或关联性，选择与输入最相关的案例作为提示的基础。 




以下是使用Python编写的代码示例：

```python
from langchain.prompts.example_selector.base import BaseExampleSelector
from typing import Dict, List
import numpy as np

class CustomExampleSelector(BaseExampleSelector):
    def __init__(self, examples: List[Dict[str, str]]):
        self.examples = examples
    def add_example(self, example: Dict[str, str]) -> None:
        """Add new example to store for a key."""
        self.examples.append(example)
    def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:
        """Select which examples to use based on the inputs."""
        return np.random.choice(self.examples, size=2, replace=False)

examples = [ {"foo": "1"}, {"foo": "2"}, {"foo": "3"}]
example_selector = CustomExampleSelector(examples)
print(example_selector.select_examples({"foo": "foo"}))
example_selector.add_example({"foo": "4"})
print(example_selector.examples)
print(example_selector.select_examples({"foo": "foo"}))
```
#### 6.2.3 输出解析器

使用输出解析器（OutputParser）可以使模型的输出更加结构化。下面是一些常见的输出解析器及其功能。

（1）**get_format_instructions**：指示模型如何格式化输出。通过调用该输出解析器，可以获取有关如何格式化输出的指示。 

（2）**parse(str)**：将输出解析为所需的格式。该输出解析器接收一个字符串参数，并将其解析为所需的格式。 

（3）**CommaSeparatedListOutputParser**：以逗号分隔的形式返回输出。例如，将一个列表['Vanilla', 'Chocolate', 'Strawberry', 'Mint Chocolate Chip', 'Cookies and 
