

### 第2章 部署Llama 2大模型
为什么需要部署大模型？
大模型正在各行各业中应用，以提高人们的生产力和获取信息的能力。在医疗保健行业中，大模型能协助医生进行诊断和制订护理计划，同时为患者提供个性化的康复评估和诊疗咨询。在金融行业中，大模型结合本地的文档库或知识库，能完成自然、灵活的信息检索和数据搜索。大模型通过类似于人类对话的自然交互方式，使专业知识和复杂信息能被轻易获取。出于数据隐私、垂直定制、成本控制等各种原因，用户可能不会满足于仅通过付费接口来调用“黑盒”式的大模型服务，开源的大模型给我们提供了灵活微调与本地部署的便利。本章将探讨如何部署开源的Llama 2大模型。
### 2.1 部署前的准备
部署大模型前应确保有足够的下载模型权重的硬盘空间，合适的CPU、内存大小，以及NVIDIA（英伟达）GPU型号。
### 2.1.1 硬件准备
部署不同参数的Llama 2大模型对GPU的要求如下表所示。
### 表2-1 部署不同参数的Llama 2大模型对GPU的要求
|模型名称|模型加载名称|基础模型版本|模型权重大小|
| ---- | ---- | ---- | ---- |
|Llama2-Chinese-7b-Chat|FlagAlpha/Llama2-Chinese-7b-Chat|meta-llama/Llama-2-7b-chat-hf|13.50GB|
|Llama2-Chinese-13b-Chat|FlagAlpha/Llama2-Chinese-13b-Chat|meta-llama/Llama-2-13b-chat-hf|26.03GB|

本次我们要部署Llama2-Chinese-7b-Chat模型，其对显卡的要求如下表所示。
### 表2-2 Llama2-Chinese-7b-Chat模型对显卡的要求
|模型名称|显存需求|显卡型号|
| ---- | ---- | ---- |
|Llama2-Chinese-7b-Chat|>12GB|RTX 2060 12GB，3060 12GB，3080，A2000及以上|

### 2.1.2 环境准备
Python版本：3.9.6及以上。
可以通过在命令行中运行以下命令来检查Python版本：
```python
python --version
```
如果Python版本低于3.9.6，那么需要更新Python。如果系统中没有安装Python，或者想在一个独立的环境中安装新的Python版本，那么可以考虑使用像Anaconda或pyenv这样的Python版本管理工具。
Anaconda是一个流行的Python和R的发行版，包含Conda（一个包和环境管理器），以及大量的科学计算库。使用Anaconda可以轻松地在同一个系统中管理和切换多个Python版本与环境，Anaconda非常适用于科学计算和数据分析。
下面是使用Anaconda安装Python的步骤。
1. 访问Anaconda的官方网站 下载Anaconda。
2. 根据操作系统（Windows、macOS或Linux）和系统类型（32位或64位），选择合适的Anaconda版本进行下载。Anaconda默认包含最新版本的Python，因此不需要另外选择Python版本。
3. 下载完成后，打开安装包并按照安装向导的提示进行安装。
    - Windows用户：双击下载的exe文件开始安装。
    - macOS用户：双击下载的pkg文件开始安装。
    - Linux用户：在终端中，进入下载文件的目录，运行bash Anaconda3-xxx.sh（其中xxx是下载的Anaconda的版本号）。
4. 在安装过程中，记得勾选或同意将Anaconda添加到PATH环境变量中。
5. 安装完成后，打开命令行或终端，运行python--version命令，如果显示Python版本号，那么说明Python已经成功安装。可以运行conda list命令来查看已经安装的Python包和库。
一旦安装了Anaconda，就可以使用conda命令来创建和管理Python环境，以及安装和更新Python包。例如，可以使用以下命令创建一个新的Python环境：
```python
conda create --name myenv python=3.9
```
在这个命令中，myenv是新环境的名称；python=3.9指定了想在这个环境中安装的Python版本。用户可以根据实际需要修改这些参数。
创建环境后，可以先使用conda activate myenv命令来激活这个环境，然后在这个环境中安装和运行Python程序。如果想退出环境，那么可以使用conda deactivate命令。

### 2.2 模型的导入与加载
本节将详细介绍如何导入并加载训练好的大模型。模型文件中储存着大模型的网络结构，以及在上游预训练任务中训练好的节点权重，所以加载完成后即可进行使用。本节还将介绍如何通过不同方式加载模型并进行推理或执行其他任务。
### 2.2.1 下载代码
导入并加载训练好的Llama 2大模型通常需要使用Python的PyTorch库。首先需要安装PyTorch库和Transformers库，这可以通过pip命令来完成。
Llama 2大模型，也就是预训练模型，权重文件格式有多种，其中最常见的包括以下几种。
1. **PyTorch的“.pth”或“.pt”格式**：这种格式由PyTorch提供，可以直接被PyTorch库读取。它用于存储模型权重，并且支持在GPU和CPU之间进行无缝切换。
2. **TensorFlow的“.ckpt”格式**：这种格式由TensorFlow提供，可以直接被TensorFlow库读取。它同样用于存储模型权重，但不支持在GPU和CPU之间进行无缝切换。 
3. **Hugging Face的“.bin”格式**：这种格式由Hugging Face的Transformers库提供，可以被Transformers库读取。它用于存储模型的所有权重，并且支持在GPU和CPU之间进行无缝切换。
此外，预训练模型权重文件通常需要与模型的配置文件（通常是一个“.json”文件）一起使用，该配置文件描述了模型的结构和超参数。在加载模型时，首先读取配置文件来构建模型的结构，其次从预训练模型权重文件中读取权重来初始化模型。
运行下面的命令从GitHub中拉取Llama2-Chinese仓库中的代码：
```python
git clone https://github.com/FlagAlpha/Llama2-Chinese.git
```
过程截图如下所示。
### 图2-1 过程截图
拉取完成后，会在当前目录下生成Llama2-Chinese仓库的目录，其结构如下图所示。
### 图2-2 Llama2-Chinese仓库的目录结构
### 2.2.2 下载模型
通过访问Llama中文社区获取Llama 2大模型，可以手动根据文件链接直接下载对应的模型，也可以通过Hugging Face加载托管在其平台的模型权重。请注意，由于模型文件通常较大，因此请确保网络连接顺畅并耐心等待。可以根据下表中的模型信息选择并加载对应的模型。
### 表2-3 模型信息
|模型名称|模型加载名称|基础模型版本|
| ---- | ---- | ---- |
|Llama2-Chinese-7b-Chat|FlagAlpha/Llama2-Chinese-7b-Chat|meta-llama/Llama-2-7b-chat-hf|
|Llama2-Chinese-13b-Chat|FlagAlpha/Llama2-Chinese-13b-Chat|meta-llama/Llama-2-13b-chat-hf|
|Llama2-Chinese-7b-Chat-LoRA|FlagAlpha/Llama2-Chinese-7b-Chat-LoRA|meta-llama/Llama-2-7b-chat-hf|
|Llama2-Chinese-13b-Chat-LoRA|FlagAlpha/Llama2-Chinese-13b-chat-LoRA|meta-llama/Llama-2-13b-chat-hf| 
