通过Hugging Face调用托管模型的示例代码如下：

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained('FlagAlpha/Atom-7b', device_map='auto', torch_dtype=torch.float16, load_in_8bit=True)
model = model.eval()
tokenizer = AutoTokenizer.from_pretrained('FlagAlpha/Atom-7b', use_fast=False)
tokenizer.pad_token = tokenizer.eos_token

input_ids = tokenizer(['<s>Human: 如何向一个六岁的孩子解释什么是机器学习？\n</s><s>Assistant: '],
return_tensors="pt", add_special_tokens=False).input_ids.to('cuda')

generate_input = {
    "input_ids": input_ids,
    "max_new_tokens": 512,
    "do_sample": True,
    "top_k": 50,
    "top_p": 0.95,
    "temperature": 0.3,
    "repetition_penalty": 1.3,
    "eos_token_id": tokenizer.eos_token_id,
    "bos_token_id": tokenizer.bos_token_id,
    "pad_token_id": tokenizer.pad_token_id
}

generate_ids = model.generate(**generate_input)
text = tokenizer.decode(generate_ids[0])
print(text)
```

上述代码中通过向大模型提问“如何向一个六岁的孩子解释什么是机器学习？”，测试大模型针对该问题的推理能力。下面我们查看对应的推理效果：

```python
Python (Llama2) [root@localhost Llama2-Chinese]# python demo.py
CUDA SETUP: Loading binary
/root/miniconda3/envs/Llama2/lib/python3.10/site-
packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so...
Loading checkpoint shards: 3/3 [00:15<00:00, 100%]
5.28s/it]
<s><unk>: 如何向一个六岁的孩子解释什么是机器学习？
</s><s>Assistant: 向一个六岁的孩子解释什么是机器学习可能需要用简单的语言和例子来说明。你可以这样解释：
“你知道电脑吗？它们就像是很聪明的机器，可以帮助我们做很多事情。机器学习就是让电脑变得更聪明的一种方法。就像我们学习新东西一样，电脑也可以通过机器学习来学习新东西。
想象一下，如果你给电脑看一堆图片，有一些是猫的图片，还有一些是狗的图片。一开始，电脑不知道怎么区分猫和狗，但是，通过机器学习，它可以学会看到一些特别的东西，比如猫的耳朵形状或者狗的尾巴形状。然后，当你给电脑看一个新的图片时，它可以用之前学到的知识来猜测这是一只猫还是一只狗。
这就像是你学习认识新朋友一样。一开始你可能不知道他们的名字，但是通过和他们交谈，你会了解到一些特别的东西，比如他们的笑容或者他们的声音。然后，当你再见到他们时，你可以通过这些特征来认出他们是谁。
所以，机器学习就是让电脑通过观察和学习，变得更聪明和有能力做一些事情，就像我们通过观察和学习来认识新朋友一样。”
```

希望这个解释能帮助你向一个六岁的孩子解释什么是机器学习！

若读者运行demo程序也能看到大模型针对问题的推理效果，则表明读者的环境已经可以正常运行大模型项目。至此，Llama 2大模型的部署与测试完成，下面介绍Llama 2大模型的其他部署方式。

### 2.3 模型部署

由于大模型推理时需要采用简单的交互界面进行对话，因此本节介绍两种模型部署方式，即API部署和text-generation-webui部署，这两种模型部署方式都适用于大模型的本地部署，读者可根据自己的设备与喜好进行选择。

### 2.3.1 API部署

API部署是一种常见的模型部署方式，它允许我们通过网络请求与模型进行交互。本节将使用FastAPI和Uvicorn这两个库来部署模型。

为了完成API部署，需要安装FastAPI库和Uvicorn库。这两个库都可以通过pip命令进行安装：

```python
pip install fastapi uvicorn
```

安装完这两个库后，可以运行仓库中的accelerate_server.py脚本启动API服务器：
```python
Python accelerate_server.py \
--model_path /path/Llama2-Chinese-13B \
--gpus "0" \
--infer_dtype "int8" \
--model_source "llama2_chinese"
```

在这个命令中，使用了以下几个参数。

- **model_path**：这个参数指定了模型的本地路径。需要将其替换为Llama 2大模型文件所在的路径。

- **gpus**：这个参数指定了需要使用的显卡编号。如果希望使用编号为0的显卡，那么可以将此参数设置为“0”；如果希望同时使用编号为0和1的显卡，那么可将此参数设置为“0,1” 。

- **infer_dtype**：这个参数指定了模型加载后的参数数据类型。可以选择的数据类型包括int4、int8和float16。这个参数的设置会影响模型的运行效率和精度。 

- **model_source**：这个参数指定了模型的源。可以选择的源包括llama2_chinese和llama2_meta。应该根据下载的模型的类型来选择合适的源。例如，下载的是FlagAlpha的模型，则应该将此参数设置为“llama2_chinese”。

运行这个命令后，API服务器默认部署在本地的8001端口，并通过POST方法进行调用。可以使用accelerate_client.py脚本向API服务器发送请求：

```python
python accelerate_client.py
```
这个脚本会向API服务器发送一个请求，并打印API服务器的响应。用户可以修改这个脚本，使其发送自己的请求。

### 2.3.2 text - generation - webui部署

text-generation-webui是基于Gradio开发的可视化工具包，主要用于运行大模型的会话交互。以下是text-generation-webui的一些简单特性。

- 支持3种交互模式：分列模式、Notebook模式和聊天模式。

- 支持多种模型后端：Transformers、llama.cpp、ExLlama、AutoGPTQ、GPTQ-for-LLaMa、CTransformers等。

- 快速切换不同模型的下拉菜单。

- LoRA：即时加载和卸载LoRA，使用QLoRA训练新的LoRA。

- 精确的聊天模式指令模板：Llama2-Chat、Alpaca、Vicuna、WizardLM、StableLM等。

- 支持Transformers库进行4位、8位及CPU推理。

- 支持Transformers采样器的llama.cpp模型（llamacpp_HF加载器）。

- 多模态管道：LLaVA和MiniGPT-4。

- 支持LaTeX渲染的Markdown输出，如可以与Galactica一起使用。

- 自定义聊天角色。

- 高效的文本流式处理。
- 
1. **直接安装text-generation-webui**
   
text-generation-webui对计算机配置要求很低，CPU和6GB显存的显卡均可安装，安装方法也非常简单。
    
    - 打开text-generation-webui的项目页面。

![image](https://github.com/user-attachments/assets/52294daf-ea71-4753-a94d-6f2ca5e3f8f9)

   
    - 找到One-click installers，根据自己的操作系统下载对应的一键安装包。

    ![image](https://github.com/user-attachments/assets/97a0f50e-186f-4189-9ce2-051bcaec4478)

    
    - **解压**：把该安装包解压到需要安装的磁盘目录下，双击Start Windows bat解压。
    
    - **选择GPU类型**：在安装过程中会提示选择GPU类型，输入对应的字母即可，如果没有显卡，则可以选择CPU选项。
    
    注意：text-generation-webui的Windows版本和AMD系列显卡不兼容。

![image](https://github.com/user-attachments/assets/d73596c1-3945-4a19-86ed-63ab0d178d53)


2. **通过Conda安装text-generation-webui**

除了直接安装，还可以通过Conda进行text-generation-webui的安装，下面简单介绍安装命令和步骤。
    
    - **安装Conda**：访问Conda官方网站，下载并安装Conda，或者在Linux/WSL系统中，通过以下两条命令来进行自动安装：

```python
curl -sL "https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh" > "Miniconda3.sh"
bash Miniconda3.sh
```
    - **安装text-generation-webui**，安装命令如下：
    
```python
# 创建一个新的conda环境
conda create -n textgen python=3.10.9
conda activate textgen
```
### 2.3.3 使用text-generation-webui

1. **准备模型权重**

首先，将下载后的LoRA权重及Hugging Face格式的llama-7B模型权重分别放到loras、models文件夹下，目录文件如下：
```python
ls loras/Llama2-7b
adapter_config.json  adapter_model.bin  special_tokens_map.json  tokenizer_
config.json  tokenizer.model

ls models/llama-7b-hf
pytorch_model-00001-of-00002.bin pytorch_model-00002-of-00002.bin
config.json
pytorch_model.bin.index.json generation_config.json
```

其次，复制LoRA权重的tokenizer.model到models/llama-7b-hf下，并修改/modules/LoRA.py文件（text-generation-webui默认从models下加载tokenizer.model，因此需要使用扩展中文词表后的tokenizer.model并修改原始llama-7b-hf的embed_size）：

```python
cp loras/Llama2-7b/tokenizer.model models/llama-7b-hf/
cp loras/Llama2-7b/special_tokens_map.json models/llama-7b-hf/
cp loras/Llama2-7b/tokenizer_config.json models/llama-7b-hf/
```

修改/modules/LoRA.py文件只需在PeftModel.from_pretrained()方法之前添加代码：

```python
shared.model.resize_token_embeddings(len(shared.tokenizer))
shared.model = PeftModel.from_pretrained(shared.model,
path(f"{shared.args.lora_dir}/{lora_name}"), **params)  # 该行源代码中就有，无须改动
```

2. **加载模型并启动text-generation-webui**

运行以下命令即可与Llama 2大模型对话：

```python
python server.py --model llama-7b-hf --lora Llama2-7b --cpu
```

对话截图如图2-6所示。

### 图2-6 对话截图


提问：我要拍一个做红烧肉的短视频，请给我一个拍短视频的脚本
回答：好的！以下是一份关于如何拍摄制作红烧肉的视频脚本：
场景设置：选择一间宽敞明亮的厨房作为背景。墙壁上挂一张烹饪工具清单和一张食谱卡片。桌子上摆放着一些食材、调料和一些餐具（如刀叉）。
1. **开场镜头**：展示整个房间的背景和气氛，突出厨房的氛围感。可以加入音乐或音效来增加氛围。
2. **导入主角介绍**：首先引入视频中的主角——一位厨艺高超的女士/男士。她站在餐桌前微笑着向观众打招呼并自我介绍她的名字和你们将要一起制作的美食的名称“红烧肉”。
3. **材料准备与切菜过程**：女士开始详细说明所需要的食材以及它们的数量和质量要求。然 
