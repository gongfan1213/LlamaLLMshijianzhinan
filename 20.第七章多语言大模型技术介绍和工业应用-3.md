### 第7章 多语言大模型技术介绍及其工业应用
随着Llama 2大模型的出现，围绕多语言大模型的研究在国内外社区火热展开。本章将从多语言大模型的研究现状和技术方向出发，系统地总结多语言大模型的训练资源和预处理、优化方向，以及多语言大模型现阶段典型的工业应用。

#### 7.3 多语言大模型的优化方向
介绍完多语言大模型的预训练资源和评测任务，本节将介绍数据预处理流程和Tokenizer，以及各类多语言大模型的结构和效果。

##### 7.3.1 数据预处理流程
多语言大模型的预训练数据清洗和单语言大模型并没有显著的不同，都要经历数据的收集、初次数据清洗（语言检测）、文档去重及质量过滤（隐私过滤）等步骤，唯一的区别是，在初次数据清洗过程中，需要设置合理的阈值，识别出我们需要的语言并保留一定比例的多语言文档用于模型训练。

|流程步骤|具体内容|
| ---- | ---- |
|Collection|Downloading、Text Extraction、Site Development (URL Based)|
|Initial Cleaning|Language Identification、Threshold based filtering、Multi-language documents|
|Deduplication|Recent Substring based (nc4, OSCAR*, CC100)、Fuzzy Minhash based (GPT-3, ThePile)、Both (Refined Web)|
|Filtering|Heuristics Based (Refined Web)、Model Based (CC-Net, CC100)、NSPW URL Based, Pi、Line Based, Doc based|

##### 7.3.2 Tokenizer

经历过数据预处理的语料，需要经过Tokenizer的处理后才可以变成多语言大模型直接可用的数据格式，下面介绍常用的Tokenizer及其在多语言上的潜在问题。

1. **Tokenizer选择**

主流的Tokenizer一共分为两种：
    - **基于子词的Tokenizer**：
        - **BPE算法**：BPE算法会学习如何将两个最常见、连续并且在训练字典中的token合并，将其作为新的token加入字典，直到字典容量满足要求为止。这个算法也被GPT、BLOOM和Llama系列模型使用。
        - **Sentencepiece**：Sentencepiece是一个开源的代码库，实现了如BPE、UnigramTokenization等Tokenization算法，同时也实现了非子词的Tokenizer，如基于字和字母的Tokenizer。XGLM、mT5、PaLM系列模型都使用Sentencepiece作为Tokenizer。
    - **基于字节的Tokenizer（BBPE）**：当字典容量有限时，以字节为最小token进行分词，这种分词方法可以编码所有可能的序列信息，并且对拼写有很好的容错性。有研究使用UTF-8的编码训练了一个没有子词的多语言大模型mT5，在多语言测试集上展现出了较好的效果。这种分词方法可以用来训练没有子词的大模型，如拉丁语系语言的模型，因为UTF-8编码拉丁语系语言的长度会比编码汉语、日语和韩语要小。但是这种分词方法需要更深的编码器，并且要求大模型可以接收更长的上下文信息。Llama 2大模型对于非UTF-8的字符就使用这种分词方法。


2. **Tokenizer在多语言上的潜在问题**

与单语言大模型相比，多语言大模型更加注重Tokenizer的质量，因为多语言大模型要在词表有限的情况下利用token表示更多的语言序列。有研究从子词的分裂度（Fertility）计算一个自然词被分裂成子词的平均长度，最小的分裂度是1，表示每个自然词就是一个子词。多语言大模型mBERT的分裂度在英语（EN）、汉语（ZH）、印尼语（ID）和日语（JA）上与单语言大模型基本相同，但是在韩语（KO）、阿根廷语（AR）、芬兰语（FI）、俄罗斯语（RU）和土耳其语（TR）上明显高于单语言大模型。这是因为mBERT的训练语料中包含大量的英语等高资源语言，而英语与阿根廷语、芬兰语、俄罗斯语和土耳其语等相比，是一种弱词形变化语言，当Tokenizer在多种语言间使用同一个词表时，会导致Tokenizer不能在对应的低资源语言中学到最优的分词组合。

![image](https://github.com/user-attachments/assets/171f2ec3-ba2c-420a-902f-253d7a893f81)


随着针对某些语言子词的分裂度增大，在低资源语言上，利用基于提示的ICL调用多语言大模型进行预测，会大大增加使用成本，因为会将序列分解成更多的token，增加上下文和输出中token的数量。如果以英语或拉丁语系语言为基准，拉丁语系语言的大模型调用花费最小，其他语言的大模型调用花费一般都高于英语，这是因为Tokenizer并没有在对应语言上被很好地训练。除上述可以量化的评价以外，Tokenizer对于一些需要进行分词的语言，如汉语、日语和泰语等也会产生问题。

![image](https://github.com/user-attachments/assets/27da7dfa-2601-4336-a6b6-273e0de99fa7)


##### 7.3.3 训练数据格式和采样策略

在结束对训练语料的分词处理以后，将它们送入多语言大模型之前，多语言大模型还需要考虑如何组织多语言的样本，以及对不同语言样本的采样策略。一般情况下，使用多语言平行句对训练的大模型，在多语言的下游任务中的效果会优于仅使用单语言样本或以英语为中心的平行句对训练的大模型。对语言进行采样，是为了更好地平衡模型规模和语言规模对多语言大模型效果的影响，避免出现多语言诅咒现象。

![image](https://github.com/user-attachments/assets/add1af10-3d99-409e-aef5-601bc65fa71c)


1. **单语言样本**

单语言样本是指在训练过程中将每种语言的语料独立作为预训练任务的采样样本，通过不同的采样策略从文档池中对不同语言进行采样，形成一个“Batch”进行迭代训练。GPT-3、Llama系列、BLOOM使用的就是这种训练数据格式。多语言采样策略有三种：
    - **经验分布采样**：不做任何限制，直接根据语言样本数量形成的经验分布$p_{l}$进行采样。其中，$p_{l}$代表的是第$l$种语言的样本数量，计算公式为$p_{l}=\frac{n_{l}}{\sum_{l^{\prime}\in L}n_{l^{\prime}}}$ 。
    - **基于温度的采样策略**：一般用于少于10亿个参数的模型。这种采样策略会先使用一个采样函数$q_{l}$和一个温度参数$\tau$正则化原来的$p_{l}$分布，通过设置不同的温度确保采样时可以对低资源语言进行上采样，对高资源语言进行下采样，从而平衡训练语料中不同语言之间的比例。计算公式为$q_{l}=\frac{p_{l}^{1/\tau}}{\sum_{l^{\prime}\in L}p_{l^{\prime}}^{1/\tau}}$ 。
    - **Unimax方法**：2023年ICLR提出，希望能根据每种语言的语料中的字符数量更平均地分配训练预算（Budget），从而不用人为决定$\tau$的值，为实验带来随机性。这种方法会先以低资源语言来进行训练预算的分配，也就是组成了Batch，然后进行模型训练。实验证明，这种方法的效果要优于基于温度的采样策略。

2. **以英语为主的平行句对**

以英语为主的平行句对的训练数据格式及模型对照，以英语作为中间语言，采样与该样本对齐的目标语言语料组成平行句对，将其输入模型进行Masked LM任务预训练。这种方法借助了机器翻译的思想，希望Masked不同语言的token信息，使模型学习到不同语言与英语之间的关系。如果需要应用上文提到的采样策略，则需要以句对（如中英句对）为最小统计单元进行采样。

![image](https://github.com/user-attachments/assets/a437856d-b3f7-4909-9476-f6c164697720)


3. **多语言平行句对**

与以英语为主的平行句对方法相比，多语言平行句对不再以英语作为中间语言，以保证模型在训练过程中可以学习到不同语言之间的直接关系。因为英语的弱词形变化特征决定了它并不一定是一种好的桥梁语言。2022年10月，使用这种方法训练的模型T-ULRv6，在之前提到的多语言Benchmark XTREME和单语言GLUE上纷纷取得了SOTA水平，证明多语言大模型可以同时在英语及多语言评测任务上取得最佳效果。

##### 7.3.4 多语言大模型的训练任务

多语言大模型的训练任务分为预训练任务和微调任务。本节将介绍多语言大模型的预训练任务，主要关注两个方面：多语言大模型的训练目标构造和多语言大模型结构。



1. **多语言大模型的训练目标构造**

在介绍多语言大模型结构之前，先简单介绍三种训练目标构造方法。不同语言大模型的训练目标通过覆盖输入的不同部分形成loss值，随后利用loss值更新模型的参数。
    - **Regular-Denoising（R-Denoising）**：一般用于MLM的掩码策略，训练目标通常是先将一定范围内的token替换为独立的Mask，然后训练模型去预测它，如BERT。
    - **Specific-Denoising（S-Denoising）**：这个方法会把给定的句子分成前缀和后缀两部分，前缀部分作为context，后缀部分作为训练目标，最常用于文本生成和机器翻译任务。
    - **Extreme-Denoising（X-Denoising）**：这种方法可以看作在Regular-Denoising方法基础上，将掩码的窗口由token变成句子，最后的训练目标要预测每一段被覆盖的句子。这种方法在训练多语言大模型时通常为多轮对话任务所采用。在实际应用中，Llama 2只使用了第二种方法，Llama2-Chat使用了第二种方法和第三种方法，Google的PaLM 2使用了三种方法，通过在每个输入前加入R-S-E标志来确定不同的训练目标。

2. **多语言大模型结构**
Transformer是目前最常使用的多语言大模型结构，主要由编码器和解码器两个结构组成。从两者的组合方式来看，可以分为编码器、编码器-解码器及前缀语言模型（Prefix LM）三种结构。早期BERT使用的是编码器结构，现在编码器已经很少被独立使用了。
    - **解码器结构**：这种结构是现在主流的多语言大模型所使用的结构，让模型可以像传统的自回归语言模型一样被训练，即利用单向注意力机制进行NTP。Llama 2、GPT系列、BLOOM使用的就是这种结构。最近有研究提出，在解码器结构的模型上进行去解码器的训练或利用指令微调，也能提升模型效果。但是这些方法都是基于以英语为主的模型的，在多语言大模型上的效果并不清楚。PaLM 2在其报告中使用类似的方法在多语言大模型任务中取得了出色的成绩，所以可以将其作为多语言大模型优化的一个方案。
    - **编码器-解码器结构**：这种结构使用的是Transformer的编码器及解码器。先将token的序列输入编码器，得到与输入相同长度的序列向量，然后将其作为解码器的输入。解码器使用交叉的注意力机制，针对输入使用双向注意力机制，可以关注输入的所有上下文信息。双向注意力机制是一个能高效利用数据的策略，因为它在预测token时可以利用这个token前后的信息。但是这种方法更善于执行自然语言理解任务而不是现在大模型所执行的自然语言生成任务，所以在大模型中较少独立使用。针对输出序列，使用单向注意力机制，以防止模型关注预测token之后的信息。在多语言大模型中，mT5还在继续使用这种结构。
    - **前缀语言模型结构**：本质上还是解码器结构，但是更改了解码器结构中的注意力机制，针对输入序列前缀的token使用双向注意力机制，而其他的token使用单向注意力机制。这样前缀语言模型就可以对输入序列采用双向注意力机制，以尽可能多地收集语言信息，而输出序列采用自回归方式进行逐一预测，并且同时在编码和解码的过程中共享参数。与之前提到的编码器-解码器结构相比，这种结构将编码器的编码工作交由解码器来完成，在保证计算量相似的同时，节省了2倍的参数及内存。一般情况下，不会利用这种结构从头开始训练多语言大模型，而会先训练解码器结构的模型，然后将任务转化成符合前缀语言模型输入格式的语言，对模型进行二次训练，加快收敛速度。常见的前缀语言模型有Llama2-Chat、GLM-130B和U-PaLM。

![image](https://github.com/user-attachments/assets/4cf12504-efa8-45e5-82c2-540d17cc4f76)

![image](https://github.com/user-attachments/assets/2c5ee53d-9c63-4b63-80b8-7fe329bda5dd)


![image](https://github.com/user-attachments/assets/217dd4a6-c990-46b2-9c89-a96bb129e75e)


![image](https://github.com/user-attachments/assets/3ad4fac5-6d6e-4888-8811-2f1d19e2ae6c)


##### 7.3.5 多语言大模型的优化方向总结（以Llama 2为例）

本节将以Llama 2为例，综合之前介绍的技术方向，从训练语料增强、Tokenizer选择、训练语言采样和训练目标，以及多语言大模型训练和微调4个方面介绍如何利用Llama 2训练一个擅长执行多语言任务的大模型。

![image](https://github.com/user-attachments/assets/93e5fbe0-2d49-4d55-a5d2-978e10fa0759)


![image](https://github.com/user-attachments/assets/46e9690b-0079-4fda-8361-d23723c1ae9c)


1. **训练语料增强**

在Llama 2的训练语料中，英语语料占89.7%，其他语言语料的占比小于2%，还有约8.38%的编程语言语料。尽管在Llama 2的技术报告中，Llama 2已经在英语数据集上达到可与GPT-3媲美的效果，但是在多语言（如汉语）的任务中，Llama 2却没有表现出很好的效果，就连Meta也表示Llama 2不适合使用在其他的语言上。因此，如果想基于Llama 2进行多语言大模型的训练，第一步就要收集更多的多语言训练语料。
    - **基于单语言的训练语料增强**：如果想提高Llama 2在除英语外的其他语言任务中的表现能力，则需要收集特定语言的语料，对Llama 2进行重训练。此外，除了收集与英语等量的语料，还可以使用收集的语料利用BPE等编码算法生成新的token，将其补充到原有的Llama 2字典中，并且开始进行模型的重训练，这样就可以提高模型在对应语言任务中的能力。
    - **基于多语言的训练语料增强**：基于单语言的训练语料增强虽然可以提高模型在特定语言任务中的能力，但是还欠缺多语言能力，无法解决模型对于高资源语言的偏置问题。多语言能力的典型应用就是机器翻译，当想训练一个具备各类语言翻译能力的多语言大模型时，需要收集足够的多语言训练语料。现阶段公开的多语言数据集，如mC4、ROOT及OPUS-100等都可以用来作为大模型的增强训练语料。

一般情况下，使用多语言平行句对训练的大模型，在多语言的下游任务中的效果会优于仅使用单语言样本或以英语为中心的平行句对训练的大模型，所以在构造多语言训练语料时，建议构造多语言平行语料，并且利用数据增强平衡平行语料中互译句对的数量，使其尽量相似，如使英语翻译为汉语的语料与汉语翻译为英语的语料数量尽量相似。

当完成训练语料的增强后，如果想继续使用新的token增强字或训练自己的分词器，则建议利用XLM-R的分词器作为基准，评测一下自己的分词器在不同语言下的分裂度。因为XLM-R的字典中特意增加了对小语种token的支持，可以很好地解决模型在训练时对高资源语言的偏置问题，所以如果分词器对于不同语言的分裂度和XLM-R基本相似或更小，那么说明模型可以很好地解析此类语言特征，并且能提高在推理和训练时的效率。




2. **Tokenizer选择**

一般情况下，会使用在SentencePiece中的BPE算法进行模型训练。需要注意的是，大部分模型（包括Llama 2）在训练时都会将表示数量的字符拆为单独的数字，以增强对数学问题的回答能力。对于不同的语言，可能会有与阿拉伯数字不一样的表达形式，虽然数据量不会很大，但是细心的读者也可以将其变换为阿拉伯数字，进行统一处理。同时Llama 2也会将不认识的字符利用BBPE分解成Byte，以提高对少见字符的覆盖率（如Emoji等），读者直接复用即可。


3. **训练语言采样和训练目标**

当准备好训练语料，准备重训练Llama 2时，还需要注意训练语言采样策略，其中包括基于温度的采样策略和Unimax方法。如果选用基于温度的采样策略，则建议温度系数使用3.33，这也是XLM-R通常使用的。



除了要对训练语言采样，还要根据训练目的选择合适的模型结构和训练目标。如果希望训练一个基于Llama 2的多语言大模型，那么可以使用解码器和NTP技术进行训练；如果只是希望利用已有语料针对多语言任务进行微调，那么可以选择使用前缀语言模型结构，并且搭配不同的解码目标，如针对多语言的指令微调，可以选择Specific-Denoising作为训练目标，只针对答案计算损失；如果想训练一个支持多轮交互的Chatbot，那么需要采用Extreme-Denoising作为训练目标，因为只需要计算每一轮的Chatbot回答的损失，而不需要考虑用户问题的损失，尽管模型会看到并在Transformer层关注所有的上下文信息。

4. **多语言大模型训练和微调**
当明确了自己的目标（重训练或微调目标）后，准备好训练语料、Tokenizer字典、训练目标和超参数，就可以针对Llama 2进行重训练或微调了。
    - **基于Llama 2的多语言重训练**：一般模型的重训练会使用常见的解码器结构，训练目标也是预测下一个字符，但是在多语言训练中，为了避免模型过多地拟合高资源语言，进而妨碍对低资源语言的学习，通常会采用课程学习（Curriculum Learning）方法来进行多语言大模型的预训练，希望能将一些通用的知识从高资源语言迁移到低资源语言，并且保留模型在高资源语言上的出色能力。一般的做法是将训练分为两个阶段：第一个阶段以高资源语言训练语料为主，尽量让模型学习更多的通用语言知识；第二个阶段扩大低资源语言
