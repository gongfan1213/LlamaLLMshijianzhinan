### Prefix-Tuning

Prefix-Tuning（论文“Prefix-Tuning: Optimizing Continuous Prompts for Generation”）的主要思想是通过微调预训练模型使其适应特定任务。在微调过程中，模型会学习如何根据给定的前缀生成合适的文本。在输入token之前构造一段与任务相关的virtual tokens作为Prefix。此外，在训练时只更新Prefix部分的参数，而模型中其他部分的参数固定。

与其他的参数高效微调方法不同，Prefix-Tuning不需要大量标注数据，因为它可以通过少量的前缀和待生成的文本来进行微调。微调（顶部）更新所有Transformer参数（粉红色Transformer框），并且需要为每个任务存储完整的模型副本。我们建议进行前缀调优（底部），它冻结Transformer参数，并且仅优化前缀（粉红色Prefix块）。

![image](https://github.com/user-attachments/assets/1f4a66be-2b84-4646-a1e9-ff712fd8a6d4)


### 图3-1 Prefix-Tuning
从图3-1中可以看到，Transformer参数完全固定，我们只需要对Prefix部分进行训练，对于不同的任务训练不同的Prefix，在实际使用时，挑选与任务相关的Prefix和Transformer进行组合。

### Adapter-Tuning
Adapter-Tuning是一种用于NLP任务的参数高效微调方法，它可以在不改变预训练模型结构的情况下，通过微调适配器来适应特定任务。适配器是一种轻量级的神经网络模块，从图3-2中可以看到，它可以在预训练模型的中间层添加，用于处理特定任务的输入和输出。

![image](https://github.com/user-attachments/assets/146e9e4a-00ed-4421-bbf1-0c2e139deb85)


### 图3-2 Adapter-Tuning

在Adapter-Tuning中，适配器被添加到预训练模型的中间层，用于处理特定任务的输入和输出。适配器的参数可以通过微调来进行优化，以适应新的数据分布。与传统的微调方法不同，Adapter-Tuning只微调适配器的参数，而非整个模型的参数。这种方法可以大大降低微调的计算成本、减少微调时间，同时可以保留预训练模型的大部分参数，从而提高模型的泛化能力。

Adapter-Tuning的优点是可以在不改变预训练模型结构的情况下，快速适应特定任务。由于适配器是轻量级的模块，因此可以在不占用太多计算资源的情况下进行微调。此外，适配器还可以在不同的任务之间实现共享，从而降低微调的计算成本、减少微调时间。


然而，Adapter-Tuning也存在一些限制。首先，适配器的性能取决于预训练模型的质量和适配器的设计。其次，适配器只能处理特定任务的输入和输出，因此对于一些需要处理多个任务的场景，可能需要添加多个适配器。最后，适配器的性能可能会受到预训练模型的限制，因此在一些复杂的任务中，可能需要使用更加复杂的微调方法。

### Prompt-Tuning

大模型的全参数微调需要为每个任务训练一个模型，这样的开销和部署成本都比较高。同时，离散的Prompt方法（人工设计Prompt，并将其添加到模型中）成本也比较高，并且效果不太好。

为了解决这些问题，Prompt-Tuning通过反向传播更新参数来学习Prompt，而非人工设计Prompt。同时，该方法冻结模型原始权重，只训练Prompt参数。训练完成后，同一个模型就可以用于多个任务的推理，从而降低了成本和部署的复杂性。

Prompt-Tuning可以看作Prefix-Tuning的简化版本，它给每个任务定义了自己的Prompt，并将其拼接到数据上作为输入。与Prefix-Tuning不同的是，Prompt-Tuning只在输入层加入prompt tokens，旨在通过添加模板的方法避免引入额外的参数，从而使模型可以在小样本或零样本场景下达到理想的效果。

同时，Prompt-Tuning还提出了Prompt Ensembling，即在一个批次（Batch）里同时训练同一个任务的不同Prompt（采用多种方式询问同一个问题）。

### P-Tuning v1
对于某些任务而言，人工设计模板并不是一件容易的事情。不同的模板在不同的模型、数据和任务上的效果差别可能很大。在这种情况下，P-Tuning v1重新审视了关于模板的定义，放弃了“模板由自然语言构成”这一常规要求，从而将模板的构建问题转换为连续参数优化问题。这种方法虽然很简单，但非常有效。

P-Tuning v1将Prompt转换为可以学习的Embedding层，并用MLP+LSTM的方式对Prompt Embedding进行处理。与Prefix-Tuning相比，P-Tuning v1加入了可微的virtual tokens，但仅限于在输入层加入，并没有在每一层都加入。另外，virtual tokens插入的位置也不一定是前缀，其插入的位置是可选的。这里的出发点实际上是把传统人工设计的模板中的真实token替换成可微的virtual tokens。

### P-Tuning v2

P-Tuning v1存在一定的局限性。研究表明，当模型规模超过100亿个参数时，P-Tuning v1的表现可以与全参数微调相媲美，但对于规模较小的模型，P-Tuning v1的表现较差。P-Tuning v1缺乏跨任务的通用性，其在序列标注任务中的有效性尚未得到验证。序列标注任务需要预测一系列无实际意义的标签，这对于P-Tuning v1来说具有挑战性。当模型层数较多时，P-Tuning v1微调过程中模型的稳定性难以保证。此外，模型层数越多，第一层输入的Prompt对后续层的影响越难以预估。

P-Tuning v2是P-Tuning v1的改进版本，同时借鉴了Prefix-Tuning。与P-Tuning v1相比，P-Tuning v2将P-Tuning v1中原本只加在输入层的、可微的virtual tokens以前缀的形式添加到每一层Transformer的输入中，层与层之间相互独立；在Prefix部分，每一层Transformer的输入都需要被微调，每一层Transformer的输入不是上一层的输出，而是随机初始化的Embedding。

在这种情况下，可以将原本P-Tuning v1中的语义标签（只能用于文本分类任务和问答任务）替换为传统微调方法使用的下游分类器，以便在机器阅读理解、命名实体识别和语义角色标注等更具有挑战性的序列标注任务上达到更好的效果。

### LoRA

随着规模的增大，模型会展现出各种能力。特别是对于大模型而言，随着规模的增大，其在零样本学习、常识推理等方面的能力会大幅提高。然而，与规模较小的模型相比，大模型的微调和部署成本都非常高。此外，如果需要针对不同的下游任务微调多个模型，就需要为每个下游任务保存一份模型权重，成本非常高。

在这个背景下，LoRA（论文“LoRA: Low-Rank Adaptation of Large Language Models”）的核心思想是通过低秩分解来模拟参数的改变量，从而以极小的参数量来实现大模型的间接训练。

LoRA的思想很简单，即在原始预训练模型旁边增加一个旁路，执行一个降维再升维的操作，来模拟所谓的intrinsic rank；训练时固定原始预训练模型的参数，只训练降维矩阵A与升维矩阵B，而模型的输入、输出维度不变，输出时将BA与预训练模型的参数叠加；用随机高斯分布初始化A，用0矩阵初始化B，保证训练开始时此旁路矩阵依然是0矩阵。

![image](https://github.com/user-attachments/assets/075d406a-38e2-4a88-bc23-43bc624f9c37)


### 图3-3 LoRA

### 3.3.2 模型参数高效微调

目前，针对大模型，主流的微调方法是P-Tuning和LoRA，故本节只对P-Tuning v1、P-Tuning v2和LoRA的模型进行详细的介绍。

1. **P-Tuning v1**

可以参考以下P-Tuning v1的核心代码进行微调：
```python
from peft import (
    get_peft_config,
    get_peft_model,
    get_peft_model_state_dict,
    set_peft_model_state_dict,
    PeftType,
    TaskType,
    PromptEncoderConfig
)
# 创建一个PromptEncoderConfig对象，指定任务类型为CAUSAL_LM，虚拟令牌数量为20，编码器隐藏层数量为128
peft_config = PromptEncoderConfig(task_type=TaskType.CAUSAL_LM,
                                  num_virtual_tokens=20, encoder_hidden_size=128)
```

这一步实现了导入相关环境和创建P-Tuning v1对应的配置。

```python
# 从预训练模型路径加载CausalLM模型
model = AutoModelForCausalLM.from_pretrained(model_name_or_path)
# 将加载的模型转换为PEFT模型，使用指定的PromptEncoderConfig
model = get_peft_model(model, peft_config)
# 打印PEFT模型中可训练参数的数量和名称
model.print_trainable_parameters()
```

这一步实现了加载模型和通过调用get_peft_model()包装加载的模型。

2. **P-Tuning v2**

可以参考以下P-Tuning v2的核心代码进行微调：

```python
from peft import get_peft_config, get_peft_model, PrefixTuningConfig, TaskType, PeftType
# 创建一个PrefixTuningConfig对象，指定任务类型为CAUSAL_LM，虚拟令牌数量为30
peft_config = PrefixTuningConfig(task_type=TaskType.CAUSAL_LM,
                                 num_virtual_tokens=30)
```

这一步实现了导入相关环境和创建P-Tuning v2对应的配置。其中，PrefixTuningConfig中prefix_projection参数默认值为false，表示使用P-Tuning v2，如果该参数值为true，则表示使用Prefix-Tuning。

```python
# 加载模型，通过调用get_peft_model()包装加载的模型
model = AutoModelForCausalLM.from_pretrained(model_name_or_path)
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
```

3. **LoRA**

可以参考以下LoRA的核心代码进行微调：
```python
from peft import get_peft_config, get_peft_model, get_peft_model_state_dict, LoraConfig, TaskType
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    lora_r=8,
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj']
)
```

上述代码表示导入相关的库和创建LoRA对应的配置。其中相关参数的说明如下。

- **task_type**：指定任务类型。

- **inference_mode**：是否在推理模式下使用PEFT模型。 

- **lora_r**：LoRA低秩矩阵的维数，一般使用8、16、32。 

- **lora_alpha**：LoRA低秩矩阵的缩放系数，是一个超参数，一般使用16、32。 

- **lora_dropout**：LoRA层的丢弃率，取值范围为[0, 1)。 

- **target_modules**：要替换为LoRA的模块名称列表或模块名称的正则表达式。针对不同类型的模型，模块名称不一样，因此需要根据具体的模型进行模块名称设置。例如，Llama 2大模型的默认模块名称为[q_proj, v_proj]，我们也可以自行指定为[q_proj,k_proj,v_proj,o_proj]。 

```python
# 加载模型，通过调用get_peft_model()包装加载的模型
model = AutoModelForCausalLM.from_pretrained(model_name_or_path)
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
```

目前，这三种微调方法评测效果最好的是LoRA。 
