
### 1. 通用数据

现有的绝大多数大模型都使用了通用数据，包括网页数据、对话数据、图书数据等，这些数据可以提供大量的文本资源，同时覆盖了各个领域。利用通用数据训练的大模型，在通用领域中有不错的表现。

1. **网页数据**

随着信息技术的发展，在如今这个数据呈爆炸式增长的时代，大模型可以获取多种多样的网络数据来增强其泛化能力，比较典型的是网页数据。在已有的研究工作中，研究人员已经从网页中爬取了大量文本数据，如Common Crawl 。这种数据的特点是数据量大、分布散乱且格式繁杂。爬取到的文本数据中往往包含高质量的文本（如维基百科中的文本）和低质量的文本（如垃圾邮件中的文本），因此对网页数据进行过滤和处理以提高数据质量非常重要。

2. **对话数据**

对话数据作为语音交互的原始载体，对于提升大模型的对话能力具有不可替代的作用。研究人员主要从公共对话语料库和在线社交媒体中收集对话数据。其中，公共对话语料库中的数据虽然较为丰富，但部分数据可能存在过时或不适应最新应用场景的问题。在线社交媒体，如PushShift.io语料库等，则为研究人员提供了一个获取实时更新的对话数据的平台。相较于网页数据，对话数据的收集和处理难度较大，而且高质量对话数据的数量相对较少。 

3. **图书数据**

与其他语料库相比，图书作为长文本书面语的数据来源，具有极高的价值。它为大模型提供了丰富的学习资源，并且有助于大模型深入理解复杂的语言现象，如语言知识、长距离的依赖关系和长文本生成等。然而，由于版权等因素的限制，可获取的开源图书数据资源相对较少。目前，研究界通常把Pile语料库中的数据集作为开源图书数据的主要来源。

### 2. 专业数据

专业数据对于提升大模型在特定下游任务中的具体表现很有帮助。

1. **多语言数据**

将不同语言的数据整合到大模型中可以显著提高其在多语言的理解和生成方面的能力。例如，BLOOM和PaLM 是两个使用了多语言数据的大模型，BLOOM整合了涵盖46种语言的数据，而PaLM整合了涵盖122种语言的数据。这种模型在多语言任务（如翻译、多语言摘要和多语言问答等）中表现出令人印象深刻的性能，并且在目标语言的单语言任务中也能达到或超过其他单语言模型的性能水平。通过整合多语言数据，大模型能够更好地理解和生成不同语言的文本，从而提高其跨语言的应用能力。这样的整合能够为大模型提供更广泛的语言知识和语境，进一步提升其在多语言环境下的表现。

2. **科学数据**

随着科学出版物的增加，人们对科学的研究和探索也在不断深入。为了提高大模型对科学知识的理解能力，将科学数据纳入大模型的预训练过程是非常有必要的。通过在大量科学数据上进行预训练，大模型可以在科学和推理任务中展现出良好的性能。为了构建科学语料库，目前的研究主要集中在收集来自arXiv论文 、科学教科书、科学网页等的数据。考虑到科学数据的复杂性，如数学符号和蛋白质序列等的复杂性，通常需要采用特定的词元切分和预处理技术，将不同格式的数据转换为大模型可以一致处理的形式。科学数据的整合使大模型能够更好地理解科学领域的文本，并且具备解决科学问题和完成推理任务的能力。通过在科学数据上进行预训练，大模型能够获得与科学相关的知识和语境，从而提高其在科学领域的性能和适用性。 

3. **代码数据**

近年来，程序生成领域的研究受到人们的广泛关注。最新的研究表明，在大规模代码数据上对大模型进行预训练，可以显著提高生成程序的质量。这种大模型在设计单元测试用例或解决竞技编程问题方面表现出色。用于预训练大模型的代码数据通常有两个主要来源，即编程问答社区（如Stack Exchange ）和公共软件库（如GitHub ）。相较于自然语言文本数据，代码数据具有编程语言的格式，其中包含长距离的依赖关系和准确的执行逻辑。预训练大模型使用代码数据能够使大模型对程序生成任务有更好的理解，并且能够生成具有准确逻辑和语法的代码，这对于提升程序生成模型的能力和性能具有重要意义。

下面以现有的研究为例说明大模型训练语料的构成。GPT-3 （参数量为1750亿个）使用了3000亿个混合数据集，包括Common Crawl、WebText2、Books1、Books2和维基百科；PaLM （参数量为5400亿个）使用了7800亿个混合数据集，包括社交媒体对话、过滤后的网页、书籍、GitHub、多语言维基百科和新闻；Llama （参数量为650亿个）使用了14000亿个混合数据集，包括Common Crawl、C4、GitHub、维基百科、书籍、arXiv和Stack Exchange。



### 1.3.2 数据处理

在收集到大量数据之后，对数据进行处理以构建训练语料库非常重要，特别是要删除噪声、冗余、不相关和潜在有害的数据，这些数据可能会影响大模型的容量和性能。

1. **数据过滤**

目前，过滤收集到的数据中的低质量数据有两种主要方法 ：基于分类器的过滤方法和启发式过滤方法。基于分类器的过滤方法需要先使用高质量数据进行分类器训练，然后将分类器应用于识别和过滤低质量数据。该方法利用分类器对数据进行判别，将被分类为低质量的数据过滤掉。启发式过滤方法通过设计一组合理的规则来过滤低质量数据。这些规则可以基于语言特征、指标、统计信息及关键字等进行设计。启发式过滤方法的例子包括BLOOM 和Gopher 等。通过采用这些数据过滤方法，可以有效地识别和过滤低质量数据，从而提高语料库的质量。

2. **去重**

为了提高大模型的训练效率，确保大模型训练的多样性和稳定性，对训练语料库进行去重是必要的。去重过程可以在不同的粒度级别上进行，包括句子级别、文档级别和数据集级别。在句子级别上，应该删除低质量的句子，包括重复的单词和短语，因为它们可能会引入重复模式，降低大模型训练的多样性。在文档级别上，可以使用统计特征之间的重叠比例（如单词和N-gram词重叠）来检测和删除具有相似内容的重复文档。通过删除重复文档，可以确保预训练数据的丰富性和多样性。在数据集级别上，为了避免发生数据集污染问题，还应从训练集中删除可能的重复文本，以防止训练集和评估集之间存在重叠。这样可以确保大模型在用于评估时能够处理未见过的数据，并提高其泛化能力。 

3. **隐私重构**

预训练数据主要来源于互联网，其中包含用户生成的内容，可能涉及敏感信息或个人信息，使用这些数据有泄露用户隐私的风险。因此，有必要对预训练数据进行处理，以删除其中的个人可识别信息（Personally Identifiable Information, PII）。一种直接而有效的处理方法是，采用基于规则的技术（如关键字检测）识别和删除包含姓名、地址、电话号码等PII的内容。采用这样的处理方法有助于保护用户隐私，确保敏感信息不会被不当使用。 

4. **词元切分**

词元切分也是数据处理的一个关键步骤。词元切分的目的是将原始文本分割成单个词元的序列，作为大模型的输入内容。在基于Transformer的大模型中广泛使用的词元切分技术通常包括字节对编码（Byte Pair Encoding, BPE）算法 、WordPiece 和Unigram 。BPE算法是1994年提出的一种通用数据压缩算法 ，之后被改编为用于NLP的词元切分算法，使用此算法的代表性大模型包括GPT 、BART 和Llama 。WordPiece最初被用于语音搜索系统，在2016年被用于神经机器翻译系统 ，在2018年被用于BERT 的词元切分。T5 和mBART 采用Unigram技术构建词元切分器。



### 1.4 大模型的预训练及微调

大模型通常需要先通过在大规模数据上进行无监督预训练学习通用知识，然后通过指令微调获取理解自然语言指令的能力，最后通过人类反馈微调使输出符合人类偏好。

### 1.4.1 无监督预训练

大模型，如GPT-3，是在大规模数据上进行训练的，以给定的前缀来进行NTP（Next Token Prediction，下一个词预测）。通过将NTP的训练目标、大规模数据集及超高参数量的模型相结合，能够产生性能极强的基座模型，该基座模型可以“理解”任何文本输入，并且可以在此基础上进行“创作”。

1. **规模定律**

规模定律 （Scaling Law）指的是模型性能与模型规模之间的渐进关系。简单来说就是随着模型规模、数据集的增大和用于训练的计算浮点数的增多，模型性能会提高。为了得到最佳模型性能，这三个因素必须同时放大。当不受其他两个因素的制约时，模型性能与每个单独的因素都有幂律关系。 

2. **语境学习**

GPT-3 的研究论文指出，大规模的训练数据催发一种新的学习范式，称为语境学习（In-Context Learning, ICL）。ICL并不需要更新模型参数，仅用几条下游任务的示例就可以取得较好的性能。



### 1.4.2 指令微调

经过预训练的大模型，已经被证明能很好地进行小样本学习。但是它们在零样本学习方面表现较差。

FLAN 提供了一种指令微调的方法，用来提高大模型的零样本学习能力。指令微调是指在通过自然语言指令描述的NLP任务数据集上，对预训练大模型进行指令调优。下图所示为Alpaca 指令微调数据集中的中文翻译样例。

### 图1-3 Alpaca指令微调数据集中的中文翻译样例

```js
{

"instruction": "给出三个保持健康的秘诀。",

"input": "",
"output": "1. 均衡饮食，多吃水果和蔬菜。\n2. 定期锻炼，保持身体活跃和强壮。\n3. 充足的睡眠，保持规律的作息时间。"
},
{
"instruction": "什么是三原色？",
"input": "",
"output": "三原色是红、蓝、黄。"
},
{
"instruction": "描述原子的结构。",
"input": "",
"output": "原子由原子核组成，原子核包含质子和中子，周围环绕着围绕原子核运行的电子。质子和中子带正电荷，而电子带负电荷，从而形成整体中性原子。每种粒子的数量决定了原子序数和原子类型。"
},
{
"instruction": "我们怎样才能减少空气污染？",
"input": "",
"output": "减少空气污染的方法有很多，如转向可再生能源、鼓励使用公共交通工具、禁止燃烧化石燃料、实施工业源减排政策及实施机动车排放标准。此外，个人可以通过减少汽车使用、避免燃烧木材等材料及改用节能电器来为减少空气污染做出自己的贡献。"
},
{
"instruction": "描述一次你不得不做出艰难决定的经历。",
"input": "",
"output": "当我在一家建筑公司担任项目经理时，我不得不做出一个艰难的决定。我负责一个需要在特定日期前完成的项目，以满足客户的期望。然而，由于意外的延误，我们无法在截止日期前完成，所以我不得不做出一个艰难的决定。我决定延后截止日期，但我不得不进一步扩展团队的资源并增加预算。尽管这是一个冒险的决定，但我最终决定继续进行，以确保项目按时完成并满足客户的期望。该项目最终成功完成，这被视为对我的领导能力和决策能力的考验。"
}
```

指令微调结合了预训练-微调及提示工程（Prompt Engineer）的研究范式，通过监督微调的方式提高大模型在推理时的自然语言交互能力。实验证明，大模型能够很好地执行通过自然语言指令描述的任务，同时评估结果也表明FLAN能够显著提高参数量为130亿个及70亿个的大模型的零样本学习能力，在很多任务中的小样本学习能力甚至超过了GPT-3。研究表明，增加指令微调的任务多样性可以提高大模型对训练时不可见任务的性能，并且指令调优只有在足够大的模型规模下才会发挥作用。

### 1.4.3 基于人类反馈微调

研究表明，持续增大模型规模并不能使模型的输出更符合人类偏好。例如，大模型可能会生成不真实、有害或对用户无益的输出，即不符合人类偏好的输出。

InstructGPT 提供了一种基于人类反馈微调（Reinforcement Learning from Human Feedback, RLHF）的方法，该方法能够在广泛的任务中使模型的输出符合人类偏好。在人工评估中，InstructGPT（参数量为13亿个）的生成结果要优于GPT-3（参数量为1750亿个）的生成结果，这表明基于人类反馈微调是使模型的输出更符合人类偏好的一种有前景的方法。这种方法主要包括以下两个步骤。

- **收集偏好数据，训练奖励模型**：从问题库中选择问题，重复生成多次回答，让标注者进行排序，利用排序数据训练奖励模型来预测人类偏好。

- **基于强化学习微调生成模型**：首先重新选择一个问题，模型生成回答，奖励模型对回答进行打分，作为标量奖励，其次利用近端策略优化（Proximal Policy Optimization, PPO）算法 通过优化奖励来微调生成模型。

这两个步骤可以连续迭代进行，通过基于当前最佳策略收集更多的偏好数据，训练出更好的奖励模型，进而训练出更好的策略。

PPO算法是一种强化学习算法，通过使奖励信号的期望最大化来微调模型的策略，使模型的输出更符合人类偏好。在微调过程中，强化学习的策略是接受Prompt并且返回文本回复。行动空间是语言模型的词表对应的所有子词单元。观察空间是所有可能的输入序列。强化学习通常会有难以训练的问题，随着微调模型的更新，强化学习产生的数据和训练奖励模型的数据的差异会越来越大，另外PPO训练会导致模型在通用任务中的性能大幅下降。为了缓解强化学习训练问题，在进行微调训练时会在PPO训练目标中加入Kullback-Leibler（KL）散度的惩罚项和通用的语言模型目标。 


### 1.5 大模型的评测
评测是大模型研究工作中不可或缺的一环，评测大模型有助于研究人员更好地分析大模型的优势和局限性，全面有效的评测结构可用于指导迭代优化模型的训练与设计。除此之外，随着大模型的广泛使用，评测大模型的安全性与鲁棒性也是至关重要的，特别是在安全敏感行业中，如金融行业和医疗保健行业 。相较于传统语言模型，大模型具有能更好地对上下文建模、可处理更复杂的语言结构，以及能更准确地遵循指令等优势。此外，随着模型规模的增大，大模型涌现出许多传统语言模型不具备的性能。因此，目前用于传统语言模型的评测方法不足以全面反映出大模型的性能。
针对主流的大模型，如ChatGPT 、PaLM 等，目前已有很多研究围绕其通用性能或特定方面的性能展开评测 。本节将分别对评测大模型不同性能所展开的特定任务、使用到的数据集和方法，以及目前大模型评测面临的挑战进行介绍。
### 1.5.1 评测任务

大模型的评测任务可以分为NLP任务与其他特定方面的性能评测任务，如鲁棒性、有毒性、偏见、人类考试、数学推理及代码编写等任务。

1. **NLP任务**

用于传统语言模型评测的NLP任务也是评测大模型性能的主要任务。NLP任务主要包括自然语言理解任务与自然语言生成任务。对于自然语言理解任务，已有较

多研究在情感分析、文本分类、信息提取等方面评测当前大模型的性能。例如，文献[8]在18个基准数据集和5个有代表性的情感分析任务中将ChatGPT与微调
后的BERT及评测任务相应的最好模型进行了比较；文献[6]使用7个细粒度信息提取任务，从ChatGPT的性能、可解释性、校准性和忠实度4个方面进行了系统的
分析。自然语言生成任务主要包括问答、机器翻译、自动摘要等。自然语言生成任务相较于自然语言理解任务更具有挑战性。例如，文献[9]发现ChatGPT在低
资源语言场景下的整体性能下降是由自然语言生成过程的性能下降而非自然语言理解过程的性能下降导致的。

2. **鲁棒性、有毒性、偏见任务**

随着大模型的广泛使用，鲁棒性、有毒性、偏见等问题的重要性日渐凸显。鲁棒性用于衡量大模型面对扰动时的稳定性，目前对于大模型鲁棒性的研究主要分为针对分布外（Out of Distribution，OOD）泛化的研究和对抗鲁棒性问题的研究。文献[10]采用AdvGLUE和ANLI基准来评测对抗鲁棒性，并采用Flipkart和DDXPlus数据集进行分布外泛化评估。在大模型广泛的预训练语料中，可能包含有毒性和偏见内容。有毒性指的是大模型在回复中无意中产生有害的、有攻击性的或不恰当的内容。偏见指的是大模型对性别、种族、宗教、职业和意识形态的明显偏向。文献[11]探究了ChatGPT面临的伦理问题、数据偏差和安全问题，同时讨论了潜在的缓解策略。 

3. **人类考试、数学推理、代码编写任务**

人类考试、数学推理、代码编写可以反映大模型与人类能力（如推理能力及工程能力）的对比结果。考试是衡量人类知识水平的常见方法，GPT-4 和PaLM 2 的技术报告中都展现了其在人类考试、数学推理、代码编写方面的表现。PaLM 2 为了体现其在多语言上的优越性，将HumanEval数据集翻译成其他编程语言进行评测，依然取得了较为不错的结果。
### 1.5.2 评测数据集

文献[1]对当前大模型的评测数据集与基准进行了较为详细的总结，并且将当前的评测数据集分为通用语言任务数据集和特定的下游任务数据集。下表所示为评测任务和相应的评测数据集。

### 表1-1 评测任务和相应的评测数据集

|评测任务|评测数据集|
| ---- | ---- |
|通用语言任务|MMLU 、BIG-Bench 、GLUE 、SuperGLUE 、HELM 、TriviaQA |
|数学工程任务|AI2 Reasoning Challenge（ARC） 、MATH 、GSM-8K 、MGSMT |
|代码编写任务|HumanEval 、MBPP 、MT-Bench |
|多轮对话任务|Chatbot Arena 、MultiMedQA |
|医学相关任务|M3Exam 、GAOKAO-Bench |
|人类考试任务|C-Eval |
|中文任务| - |

1. **通用语言任务数据集**

通用语言任务数据集包含多种NLP任务数据集、稳定性评估数据集及多任务数据集等，倾向于探究大模型的综合能力。Chatbot Arena 和MT-Bench 是评测大模型的多轮对话能力的两个重要基准。Chatbot Arena提供了一个开放平台，用户可在该平台上对两个随机分配的匿名大模型的回复进行投票。MT-Bench提供了80个高质量的多轮问题，旨在评测大模型的多轮对话和指令执行能力。GLUE 与SuperGLUE 是对大模型的自然语言理解任务，包括推理任务、相似性任务等
进行评测的基准。BIG-Bench 和MMLU 用于对大模型的知识理解和记忆能力进行评测。BIG-Bench由204项任务组成，任务主题涉及语言学、儿童发展、数学、常识推理、生物学、物理学、社会偏见、软件开发等领域。MMLU覆盖了各个级别57个学科的知识。HELM 综合了模型的7个评测指标，包括准确性、校准、鲁棒性、公平性、偏差、有毒性和效率。

2. **特定的下游任务数据集**

特定的下游任务数据集与基准涉及数学、特定语言、特定领域等方面的评测。MATH 包含12 500道具有挑战性的竞赛数学题。MultiMedQA 由7个与医学QA相关的数据集组成，专注于医学检查、医学研究和消费者保健问题，可用于医学知识评测。C-Eval 是一个中文任务数据集，其题目覆盖人文、社科、理工、其他专业四大方向，用于评测大模型的中文知识和推理能力。GAOKAO-Bench 以中国高考题目作为数据集，用于评测大模型的语言理解能力和逻辑推理能力。

### 1.5.3 评测方法

大模型的评测方法主要可分为自动评测和人工评测。与人工评测相比，自动评测通常耗费较少的时间和人力，具有效率高、迭代快的特点。但人工评测能提供更全面、更准确的真实反馈。在传统NLP任务中，通常使用BLEU 、ROUGE 等自动评测指标。近年来也有研究使用大模型辅助进行自动评测。PandaLM 是一个专门用于评测大模型性能的裁判大模型，PandaLM可作为裁判大模型实现可重复和自动化的大模型评测。文献[13]采用GPT-4和GPT-3.5作为裁判大模型，分析了裁判大模型在位置偏见、文本长度偏见等问题上的缺陷。

### 1.5.4 评测中的挑战

1. **评测数据集泄露**

由于大模型的训练数据规模十分庞大，数据集细节并不具象，并且通常不开源，因此评测数据集有可能已经包含在大模型的训练数据集中，从而导致评测的结果不准确。

2. **评测数据集覆盖不全面**

大模型多方面的性能均需要进行评测，但目前的评测数据集和基准难以覆盖全面的评测任务及各种不同的评测方法，这可能导致评测的结果不准确及不鲁棒的问题。

文献[1]中也提到了目前大模型评测面临的一些挑战，包括如何设计更鲁棒的评测、如何设计更可靠的评测，以及如何设计一个统一的、支持所有大模型的评测
框架等。
